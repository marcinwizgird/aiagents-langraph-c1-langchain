{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4568b33aafe149a6a43bbaf9b27e6c46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e784d1dc2d7243c2acbe196ef405f214",
       "IPY_MODEL_7cd67f1c9a1b463490e5619a23e56038",
       "IPY_MODEL_1efec72ac8f44d6cb55f86345b3c6d65"
      ],
      "layout": "IPY_MODEL_a874f0d2e06b4027af75e803a2b0f3e2"
     }
    },
    "e784d1dc2d7243c2acbe196ef405f214": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abc4a0f5096a4af0803c13dcdc9c8f5f",
      "placeholder": "​",
      "style": "IPY_MODEL_c42ba828e9c24aa585ecd141181cff12",
      "value": "Evaluating: 100%"
     }
    },
    "7cd67f1c9a1b463490e5619a23e56038": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fad7ebff0aa4a769a63c8c016141088",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c21d81237ff48998a1b23405dfc7232",
      "value": 48
     }
    },
    "1efec72ac8f44d6cb55f86345b3c6d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbf63bc89e1d4b8a86a8e0eaea5a8c55",
      "placeholder": "​",
      "style": "IPY_MODEL_1ee5ec2447864cbcb07d3a8e3ccd493f",
      "value": " 48/48 [03:01&lt;00:00,  3.56s/it]"
     }
    },
    "a874f0d2e06b4027af75e803a2b0f3e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abc4a0f5096a4af0803c13dcdc9c8f5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c42ba828e9c24aa585ecd141181cff12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fad7ebff0aa4a769a63c8c016141088": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c21d81237ff48998a1b23405dfc7232": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dbf63bc89e1d4b8a86a8e0eaea5a8c55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ee5ec2447864cbcb07d3a8e3ccd493f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "810c03a72a7b4c649f2efaf7ae75693e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_daf1087a3d3d4fcbb5b98f1c3d66773f",
       "IPY_MODEL_644902114067450f81bce08c76afb46f",
       "IPY_MODEL_3496d2bc3f3b4488a320658a01a1fd0d"
      ],
      "layout": "IPY_MODEL_c0ab4b24af0b4f42bec3a24b6d9d6f96"
     }
    },
    "daf1087a3d3d4fcbb5b98f1c3d66773f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6334223d95d649589919a48b7fccdd88",
      "placeholder": "​",
      "style": "IPY_MODEL_da062c135cf44ce2a641df7d18410584",
      "value": "Evaluating: 100%"
     }
    },
    "644902114067450f81bce08c76afb46f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1926fa8fabdc42e9b4207f0ce0a8b384",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ace7b465d3c4132be1ecb335a2cf970",
      "value": 28
     }
    },
    "3496d2bc3f3b4488a320658a01a1fd0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eef1de1721a54984ad2878db51d6dfc6",
      "placeholder": "​",
      "style": "IPY_MODEL_3b485fcda1c44765bcc0b7e78ea61d3f",
      "value": " 28/28 [02:46&lt;00:00, 13.16s/it]"
     }
    },
    "c0ab4b24af0b4f42bec3a24b6d9d6f96": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6334223d95d649589919a48b7fccdd88": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da062c135cf44ce2a641df7d18410584": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1926fa8fabdc42e9b4207f0ce0a8b384": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ace7b465d3c4132be1ecb335a2cf970": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eef1de1721a54984ad2878db51d6dfc6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b485fcda1c44765bcc0b7e78ea61d3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92b9550b0e814d95a1104235d82c5ebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_630cdb438b204526b719a84d16b8df02",
       "IPY_MODEL_93e370372e4644c7a9feb8f1e4a8fca9",
       "IPY_MODEL_0ba571d4dd884103a80686853cccae2a"
      ],
      "layout": "IPY_MODEL_c8ab47de54254ef58a214e4fe6af977c"
     }
    },
    "630cdb438b204526b719a84d16b8df02": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f982c542a90744ccaf0d091c3de5e244",
      "placeholder": "​",
      "style": "IPY_MODEL_90e42dee37774c8b9960dd01ff2d26a0",
      "value": "Evaluating: 100%"
     }
    },
    "93e370372e4644c7a9feb8f1e4a8fca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfde1cc7f29e4dbb8fc5cc4a0cfe5a01",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_322c1d444eed4afe996fecc2b41b615c",
      "value": 1
     }
    },
    "0ba571d4dd884103a80686853cccae2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe683a12b7784cbbbee55fc954f91966",
      "placeholder": "​",
      "style": "IPY_MODEL_290a19df62034161815cd8652f02658f",
      "value": " 1/1 [00:29&lt;00:00, 29.85s/it]"
     }
    },
    "c8ab47de54254ef58a214e4fe6af977c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f982c542a90744ccaf0d091c3de5e244": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90e42dee37774c8b9960dd01ff2d26a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cfde1cc7f29e4dbb8fc5cc4a0cfe5a01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "322c1d444eed4afe996fecc2b41b615c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe683a12b7784cbbbee55fc954f91966": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "290a19df62034161815cd8652f02658f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d71fc773eb247ac820b72fdba708049": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd94e31126b440168aa52bc678dd55b3",
       "IPY_MODEL_c52b40b4322a4bdb8556e5f3314d0669",
       "IPY_MODEL_a332124ef8b74fb4ac6253767ec24eab"
      ],
      "layout": "IPY_MODEL_ac15fe963cce46199c7fa9f1efb66222"
     }
    },
    "bd94e31126b440168aa52bc678dd55b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_722e14b1d4664ffe83631839cbc5b766",
      "placeholder": "​",
      "style": "IPY_MODEL_ef8e955c56e94a0888e85071429f47fa",
      "value": "Evaluating: 100%"
     }
    },
    "c52b40b4322a4bdb8556e5f3314d0669": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_361892c588044c58a76bf01c26af2a3d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20e22a90c2ca4f8999649f4f979ea727",
      "value": 1
     }
    },
    "a332124ef8b74fb4ac6253767ec24eab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb8204c1a2d54cad988e2b026ce80615",
      "placeholder": "​",
      "style": "IPY_MODEL_2018f8af0feb4be1b07d22d1c49b7e56",
      "value": " 1/1 [00:42&lt;00:00, 42.19s/it]"
     }
    },
    "ac15fe963cce46199c7fa9f1efb66222": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "722e14b1d4664ffe83631839cbc5b766": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef8e955c56e94a0888e85071429f47fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "361892c588044c58a76bf01c26af2a3d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20e22a90c2ca4f8999649f4f979ea727": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb8204c1a2d54cad988e2b026ce80615": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2018f8af0feb4be1b07d22d1c49b7e56": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9e0f89729fb48ceb926b1122521cc1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78e8ebfc78264af691157499da5bca18",
       "IPY_MODEL_e15ea20df727436c95a134b94c4f009a",
       "IPY_MODEL_5abafdbf8a394f81a04b72c2aee057c2"
      ],
      "layout": "IPY_MODEL_e596077fe5a24b40b95ed49f1adbad65"
     }
    },
    "78e8ebfc78264af691157499da5bca18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ddf8783380d4629bc4b6072e3c80172",
      "placeholder": "​",
      "style": "IPY_MODEL_c5f0269cd22b469196b51256a9bec3b6",
      "value": "Evaluating: 100%"
     }
    },
    "e15ea20df727436c95a134b94c4f009a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efd8428a688a4a0a87b58b386c605f33",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3f73f1dd63c41d9be1ce29cde3f0163",
      "value": 28
     }
    },
    "5abafdbf8a394f81a04b72c2aee057c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57ce375f137a45f8b3f081aba85d94bc",
      "placeholder": "​",
      "style": "IPY_MODEL_f275b1e2782d42ef8e7abb930720d746",
      "value": " 28/28 [03:28&lt;00:00, 20.35s/it]"
     }
    },
    "e596077fe5a24b40b95ed49f1adbad65": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ddf8783380d4629bc4b6072e3c80172": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f0269cd22b469196b51256a9bec3b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efd8428a688a4a0a87b58b386c605f33": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3f73f1dd63c41d9be1ce29cde3f0163": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57ce375f137a45f8b3f081aba85d94bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f275b1e2782d42ef8e7abb930720d746": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EitZYI3ZzTU0",
    "outputId": "b980215b-ea3a-44b4-d3dd-f6938809fa6b"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting dspy-ai\n",
      "  Downloading dspy_ai-3.0.4-py3-none-any.whl.metadata (285 bytes)\n",
      "Collecting ragas\n",
      "  Downloading ragas-0.4.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Collecting dspy>=3.0.4 (from dspy-ai)\n",
      "  Downloading dspy-3.0.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.0.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ragas) (0.12.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.12.3)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ragas) (1.6.0)\n",
      "Collecting appdirs (from ragas)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting diskcache>=5.6.3 (from ragas)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from ragas) (0.20.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ragas) (13.9.4)\n",
      "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (2.9.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ragas) (4.67.1)\n",
      "Collecting instructor (from ragas)\n",
      "  Downloading instructor-1.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from ragas) (11.3.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from ragas) (3.6.1)\n",
      "Collecting scikit-network (from ragas)\n",
      "  Downloading scikit_network-0.33.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (from ragas) (1.1.3)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting backoff>=2.2 (from dspy>=3.0.4->dspy-ai)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (1.5.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2025.11.3)\n",
      "Collecting optuna>=3.4.0 (from dspy>=3.0.4->dspy-ai)\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting magicattr>=0.1.6 (from dspy>=3.0.4->dspy-ai)\n",
      "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting litellm>=1.64.0 (from dspy>=3.0.4->dspy-ai)\n",
      "  Downloading litellm-1.80.10-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting json-repair>=0.30.0 (from dspy>=3.0.4->dspy-ai)\n",
      "  Downloading json_repair-0.54.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.12.0)\n",
      "Collecting asyncer==0.0.8 (from dspy>=3.0.4->dspy-ai)\n",
      "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (6.2.2)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (3.1.2)\n",
      "Collecting gepa==0.0.17 (from gepa[dspy]==0.0.17->dspy>=3.0.4->dspy-ai)\n",
      "  Downloading gepa-0.0.17-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (0.4.58)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->ragas) (0.12.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->ragas) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->ragas) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ragas) (2.19.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->ragas) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (0.17.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor->ragas) (3.1.6)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.0.0->ragas)\n",
      "  Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pre-commit>=4.3.0 (from instructor->ragas)\n",
      "  Downloading pre_commit-4.5.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting ty>=0.0.1a23 (from instructor->ragas)\n",
      "  Downloading ty-0.0.1a35-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community->ragas)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (2.0.45)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community->ragas)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->ragas) (0.4.3)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from scikit-network->ragas) (1.16.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->ragas) (3.0.0)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas)\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (0.25.0)\n",
      "Collecting fastuuid>=0.13.0 (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai)\n",
      "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ragas) (0.1.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (1.17.2)\n",
      "Collecting colorlog (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting cfgv>=2.0.0 (from pre-commit>=4.3.0->instructor->ragas)\n",
      "  Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting identify>=1.0.0 (from pre-commit>=4.3.0->instructor->ragas)\n",
      "  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit>=4.3.0->instructor->ragas)\n",
      "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting virtualenv>=20.10.0 (from pre-commit>=4.3.0->instructor->ragas)\n",
      "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->ragas) (3.3.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (1.3.10)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Downloading langchain_core-1.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->ragas)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas)\n",
      "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (4.5.1)\n",
      "Downloading dspy_ai-3.0.4-py3-none-any.whl (1.1 kB)\n",
      "Downloading ragas-0.4.1-py3-none-any.whl (419 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m419.9/419.9 kB\u001B[0m \u001B[31m37.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain_openai-1.1.3-py3-none-any.whl (84 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.6/84.6 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.7/21.7 MB\u001B[0m \u001B[31m102.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m278.2/278.2 kB\u001B[0m \u001B[31m30.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.5/45.5 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dspy-3.0.4-py3-none-any.whl (285 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m285.2/285.2 kB\u001B[0m \u001B[31m32.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
      "Downloading gepa-0.0.17-py3-none-any.whl (110 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m101.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.3/103.3 kB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.4/17.4 MB\u001B[0m \u001B[31m34.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.5/72.5 kB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.6/132.6 kB\u001B[0m \u001B[31m17.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.4/66.4 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m220.0/220.0 kB\u001B[0m \u001B[31m26.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.4/105.4 kB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.6/71.6 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading instructor-1.13.0-py3-none-any.whl (160 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m160.9/160.9 kB\u001B[0m \u001B[31m18.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m117.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m64.7/64.7 kB\u001B[0m \u001B[31m7.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading scikit_network-0.33.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.0/8.0 MB\u001B[0m \u001B[31m130.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m517.7/517.7 kB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m358.8/358.8 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading json_repair-0.54.3-py3-none-any.whl (29 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m73.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading litellm-1.80.10-py3-none-any.whl (11.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.3/11.3 MB\u001B[0m \u001B[31m147.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.9/5.9 MB\u001B[0m \u001B[31m32.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m404.7/404.7 kB\u001B[0m \u001B[31m33.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pre_commit-4.5.0-py2.py3-none-any.whl (226 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m226.4/226.4 kB\u001B[0m \u001B[31m24.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading ty-0.0.1a35-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m66.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m128.4/128.4 kB\u001B[0m \u001B[31m14.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m44.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m456.8/456.8 kB\u001B[0m \u001B[31m34.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n",
      "Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m278.1/278.1 kB\u001B[0m \u001B[31m16.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.2/99.2 kB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_core-1.2.1-py3-none-any.whl (475 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m476.0/476.0 kB\u001B[0m \u001B[31m53.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.9/50.9 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.0/6.0 MB\u001B[0m \u001B[31m157.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m469.0/469.0 kB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=278aab74b5ecf5c092509b4dc4871dd41ffe80d1c167d8524c1585e84a1d3bcc\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, magicattr, durationpy, distlib, appdirs, virtualenv, uvloop, urllib3, ty, pyproject_hooks, pybase64, opentelemetry-proto, nodeenv, mypy-extensions, mmh3, marshmallow, json-repair, jiter, identify, humanfriendly, httptools, grpcio, gepa, fastuuid, diskcache, colorlog, cfgv, bcrypt, backoff, watchfiles, typing-inspect, scikit-network, requests, pre-commit, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, asyncer, posthog, optuna, opentelemetry-semantic-conventions, onnxruntime, dataclasses-json, opentelemetry-sdk, kubernetes, instructor, opentelemetry-exporter-otlp-proto-grpc, litellm, langchain-core, langchain-text-splitters, langchain-openai, dspy, chromadb, langchain-classic, dspy-ai, langchain-community, ragas\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: opentelemetry-proto\n",
      "    Found existing installation: opentelemetry-proto 1.37.0\n",
      "    Uninstalling opentelemetry-proto-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
      "  Attempting uninstall: jiter\n",
      "    Found existing installation: jiter 0.12.0\n",
      "    Uninstalling jiter-0.12.0:\n",
      "      Successfully uninstalled jiter-0.12.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.76.0\n",
      "    Uninstalling grpcio-1.76.0:\n",
      "      Successfully uninstalled grpcio-1.76.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.37.0\n",
      "    Uninstalling opentelemetry-api-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.37.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.37.0\n",
      "    Uninstalling opentelemetry-sdk-1.37.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.1.3\n",
      "    Uninstalling langchain-core-1.1.3:\n",
      "      Successfully uninstalled langchain-core-1.1.3\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\n",
      "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
      "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed appdirs-1.4.4 asyncer-0.0.8 backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 cfgv-3.5.0 chromadb-1.3.7 coloredlogs-15.0.1 colorlog-6.10.1 dataclasses-json-0.6.7 diskcache-5.6.3 distlib-0.4.0 dspy-3.0.4 dspy-ai-3.0.4 durationpy-0.10 fastuuid-0.14.0 gepa-0.0.17 grpcio-1.67.1 httptools-0.7.1 humanfriendly-10.0 identify-2.6.15 instructor-1.13.0 jiter-0.11.1 json-repair-0.54.3 kubernetes-34.1.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.2.1 langchain-openai-1.1.3 langchain-text-splitters-1.1.0 litellm-1.80.10 magicattr-0.1.6 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 nodeenv-1.9.1 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 optuna-4.6.0 posthog-5.4.0 pre-commit-4.5.0 pybase64-1.4.3 pypika-0.48.9 pyproject_hooks-1.2.0 ragas-0.4.1 requests-2.32.5 scikit-network-0.33.5 ty-0.0.1a35 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.22.1 virtualenv-20.35.4 watchfiles-1.1.1\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": "!pip install dspy-ai ragas langchain langchain-openai chromadb datasets pandas seaborn matplotlib"
  },
  {
   "metadata": {
    "id": "Ate7AThCK9Cj",
    "ExecuteTime": {
     "end_time": "2025-12-16T14:01:42.717478Z",
     "start_time": "2025-12-16T14:01:42.701356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dspy\n",
    "import chromadb\n",
    "import ujson\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# RAGAS Imports\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from datasets import Dataset"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "630802d2",
    "outputId": "06d17af7-d87b-4059-8f28-328b0f683d6b",
    "ExecuteTime": {
     "end_time": "2025-12-16T13:59:57.466966Z",
     "start_time": "2025-12-16T13:59:57.462261Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "#from google.colab import userdata\n",
    "#import os\n",
    "\n",
    "# Retrieve the API key from Colab secrets\n",
    "#OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Set it as an environment variable so the libraries can pick it up\n",
    "#os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "#print(\"OPENAI_API_KEY loaded from Colab secrets and set as environment variable.\")"
   ]
  },
  {
   "metadata": {
    "id": "LhRvnrBOK80r"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "class RAGSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Answer the question based strictly on the provided technical documentation context.\n",
    "    \"\"\"\n",
    "    context = dspy.InputField(desc=\"technical documentation facts and retrieved snippets\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"a detailed, accurate, and faithful answer\")"
   ],
   "metadata": {
    "id": "5r6aLpxUK-PR",
    "ExecuteTime": {
     "end_time": "2025-12-16T14:04:26.311751Z",
     "start_time": "2025-12-16T14:04:26.303791Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "class DSPyRAGModule(dspy.Module):\n",
    "    \"\"\"\n",
    "    The RAG Module that connects retrieval with generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, retrieve_fn):\n",
    "        super().__init__()\n",
    "        self.retrieve_fn = retrieve_fn\n",
    "        self.generate = dspy.ChainOfThought(RAGSignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        # 1. Retrieve\n",
    "        contexts = self.retrieve_fn(question)\n",
    "\n",
    "        # 2. Format Context\n",
    "        # DSPy works best with string inputs for context\n",
    "        context_str = \"\\n\".join(contexts)\n",
    "\n",
    "        # 3. Generate\n",
    "        pred = self.generate(context=context_str, question=question)\n",
    "\n",
    "        # 4. Attach contexts to prediction under a *new* attribute for RAGAS evaluation\n",
    "        # This avoids conflict with the 'context' InputField during DSPy's internal Example construction.\n",
    "        pred.retrieved_contexts = contexts\n",
    "        return pred\n"
   ],
   "metadata": {
    "id": "MasSMNlSQ6EC",
    "ExecuteTime": {
     "end_time": "2025-12-16T14:04:27.150228Z",
     "start_time": "2025-12-16T14:04:27.139530Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 2. RAGAS Metric Adapter\n",
    "# ==========================================\n",
    "\n",
    "class RAGASMetricAdapter:\n",
    "    \"\"\"\n",
    "    Adapts RAGAS metrics to be used as optimization signals within DSPy.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def answer_correctness_metric(gold, pred, trace=None):\n",
    "        \"\"\"\n",
    "        Calculates 'answer_correctness' for a single DSPy prediction.\n",
    "        \"\"\"\n",
    "        # Extract contexts (ensure they were attached in the module forward pass\n",
    "        # under 'retrieved_contexts')\n",
    "        contexts = getattr(pred, 'retrieved_contexts', [])\n",
    "\n",
    "        # Create a mini-dataset for RAGAS\n",
    "        data = {\n",
    "            \"question\": [gold.question],\n",
    "            \"answer\": [pred.answer],\n",
    "            \"contexts\": [contexts],\n",
    "            \"ground_truth\": [gold.gold_answer]\n",
    "        }\n",
    "        dataset = Dataset.from_dict(data)\n",
    "\n",
    "        # Run RAGAS Evaluation\n",
    "        evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "        evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=[answer_correctness],\n",
    "            llm=evaluator_llm,\n",
    "            embeddings=evaluator_embeddings\n",
    "        )\n",
    "\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=[faithfulness, answer_correctness],\n",
    "            llm=evaluator_llm,\n",
    "            embeddings=evaluator_embeddings\n",
    "        )\n",
    "\n",
    "        # Extract scores (default to 0 if calculation fails)\n",
    "        f_score = results.get('faithfulness', 0)\n",
    "        ac_score = results.get('answer_correctness', 0)\n",
    "\n",
    "        # Calculate Harmonic Mean\n",
    "        # Formula: 2 * (a * b) / (a + b)\n",
    "        if f_score + ac_score == 0:\n",
    "            return 0.0\n",
    "\n",
    "        harmonic_mean = 2 * (f_score * ac_score) / (f_score + ac_score)\n",
    "\n",
    "        return harmonic_mean\n",
    "\n",
    "        # Return the float score for DSPy to optimize against\n",
    "        #return results['answer_correctness']"
   ],
   "metadata": {
    "id": "F50SzE23K-R3",
    "ExecuteTime": {
     "end_time": "2025-12-16T14:04:32.952661Z",
     "start_time": "2025-12-16T14:04:32.942278Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 3. DSPy Optimizer Class\n",
    "# ==========================================\n",
    "\n",
    "class DSPyPipelineOptimizer:\n",
    "    \"\"\"\n",
    "    Handles the compilation and optimization of the DSPy RAG Module with configurable optimizers.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer_class=BootstrapFewShot, **optimizer_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer wrapper.\n",
    "\n",
    "        Args:\n",
    "            optimizer_class: The DSPy teleprompter class to use (default: BootstrapFewShot).\n",
    "            **optimizer_kwargs: Arguments specific to the optimizer (e.g., max_bootstrapped_demos, num_threads).\n",
    "        \"\"\"\n",
    "        self.metric_fn = RAGASMetricAdapter.answer_correctness_metric\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "\n",
    "    def compile(self, module: dspy.Module, trainset: List[dspy.Example]):\n",
    "        \"\"\"\n",
    "        Compiles the module using the configured strategy and RAGAS metrics.\n",
    "        \"\"\"\n",
    "        optimizer_name = self.optimizer_class.__name__\n",
    "        print(f\"\\n⚙️  Initializing DSPy Optimizer ({optimizer_name})...\")\n",
    "        print(\"   Optimization Metric: RAGAS Harmonic Mean\")\n",
    "\n",
    "        # Instantiate the passed optimizer class with the metric and provided kwargs\n",
    "        teleprompter = self.optimizer_class(\n",
    "            metric=self.metric_fn,\n",
    "            **self.optimizer_kwargs\n",
    "        )\n",
    "\n",
    "        print(f\"   Compiling module with {len(trainset)} training examples...\")\n",
    "\n",
    "        # Some optimizers (like MIPRO) might require additional args in compile,\n",
    "        # but standard RAG optimizers usually just take the module and trainset.\n",
    "        optimized_module = teleprompter.compile(module, trainset=trainset)\n",
    "\n",
    "        print(f\"✅ Compilation with {optimizer_name} complete.\")\n",
    "        return optimized_module\n"
   ],
   "metadata": {
    "id": "Rug47cUIR4D-",
    "ExecuteTime": {
     "end_time": "2025-12-16T15:06:50.738657Z",
     "start_time": "2025-12-16T15:06:50.728245Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# 4. Main Data & Evaluation Class (Coordinator)\n",
    "# ==========================================\n",
    "\n",
    "class RAGExperimentRunner:\n",
    "    def __init__(self, collection_name=\"rag_qa_arena_tech_v3\"):\n",
    "        # Setup Chroma\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        try:\n",
    "            self.chroma_client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        self.collection = self.chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "        # Setup DSPy LM\n",
    "        self.lm = dspy.LM('openai/gpt-4o-mini')\n",
    "        dspy.configure(lm=self.lm)\n",
    "\n",
    "        # RAGAS standard evaluators for the final report\n",
    "        self.evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "        self.evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "    def _download_file(self, url, filename):\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"⬇️  Downloading {filename}...\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "    def load_and_vectorize(self, limit_docs=200):\n",
    "        # 1. Corpus (Knowledge Base)\n",
    "        corpus_file = \"ragqa_arena_tech_corpus.jsonl\"\n",
    "        self._download_file(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\", corpus_file)\n",
    "\n",
    "        print(f\"📖 Vectorizing corpus from {corpus_file}...\")\n",
    "        ids, documents, metadatas = [], [], []\n",
    "\n",
    "        with open(corpus_file) as f:\n",
    "            # Parse JSONL line by line\n",
    "            raw_data = [ujson.loads(line) for line in f][:limit_docs]\n",
    "            for idx, entry in enumerate(raw_data):\n",
    "                ids.append(str(idx))\n",
    "                # Truncate text as per original requirement\n",
    "                documents.append(entry['text'][:6000])\n",
    "                metadatas.append({\"source\": \"ragqa\"})\n",
    "\n",
    "        # Batch upsert to Chroma\n",
    "        if ids:\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(ids), batch_size):\n",
    "                self.collection.upsert(\n",
    "                    ids=ids[i:i+batch_size],\n",
    "                    documents=documents[i:i+batch_size],\n",
    "                    metadatas=metadatas[i:i+batch_size]\n",
    "                )\n",
    "\n",
    "        # 2. Test Data (Updated Source & Field)\n",
    "        qa_url = \"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\"\n",
    "        qa_file = \"ragqa_arena_tech_examples.jsonl\"\n",
    "        self._download_file(qa_url, qa_file)\n",
    "\n",
    "        print(f\"❓ Loading QA pairs from {qa_file}...\")\n",
    "        data = []\n",
    "        with open(qa_file) as f:\n",
    "            # Parse JSONL line by line\n",
    "            qa_entries = [ujson.loads(line) for line in f]\n",
    "\n",
    "            # Take small subset for demo speed\n",
    "            qa_subset = qa_entries[:10]\n",
    "\n",
    "            for entry in qa_subset:\n",
    "                # Map 'response' to 'gold_answer' as requested\n",
    "                data.append(dspy.Example(\n",
    "                    question=entry['question'],\n",
    "                    gold_answer=entry['response']\n",
    "                ).with_inputs('question'))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def retrieve(self, query: str, k=3) -> List[str]:\n",
    "        results = self.collection.query(query_texts=[query], n_results=k)\n",
    "        return results['documents'][0] if results['documents'] else []\n",
    "\n",
    "    def evaluate_system(self, module, dataset, name=\"System\"):\n",
    "        print(f\"\\n🧪 Evaluating {name}...\")\n",
    "        ragas_data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": []}\n",
    "\n",
    "        for example in dataset:\n",
    "            # Execute Module\n",
    "            pred = module(question=example.question)\n",
    "\n",
    "            ragas_data['question'].append(example.question)\n",
    "            ragas_data['ground_truth'].append(example.gold_answer)\n",
    "            ragas_data['answer'].append(pred.answer)\n",
    "            # Ensure context is captured (DSPyRAGModule attaches it)\n",
    "            ragas_data['contexts'].append(getattr(pred, 'retrieved_contexts', []))\n",
    "\n",
    "        # Run Comprehensive RAGAS Evaluation\n",
    "        # Using a subset of metrics for speed in demonstration\n",
    "        results = evaluate(\n",
    "            dataset=Dataset.from_dict(ragas_data),\n",
    "            metrics=[faithfulness, answer_correctness, context_precision, answer_relevancy],\n",
    "            llm=self.evaluator_llm,\n",
    "            embeddings=self.evaluator_embeddings\n",
    "        )\n",
    "        print(f\"📊 {name} Results: {results}\")\n",
    "        return results, Dataset.from_dict(ragas_data)\n",
    "\n",
    "    def prepare_finetuning(self, dataset, filename=\"finetune_data.jsonl\"):\n",
    "        print(f\"\\n💾 Saving fine-tuning data to {filename}...\")\n",
    "        with open(filename, \"w\") as f:\n",
    "            for row in dataset:\n",
    "                msg = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful RAG assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Context: {row['contexts']}\\n\\nQuestion: {row['question']}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": row['answer']}\n",
    "                    ]\n",
    "                }\n",
    "                f.write(ujson.dumps(msg) + \"\\n\")\n"
   ],
   "metadata": {
    "id": "udot5PSzR4MV",
    "ExecuteTime": {
     "end_time": "2025-12-16T14:04:47.568292Z",
     "start_time": "2025-12-16T14:04:47.541407Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Setup Runner\n",
    "runner = RAGExperimentRunner()\n",
    "\n",
    "# Load data (Updated to use new URL and 'response' field)\n",
    "all_data = runner.load_and_vectorize(limit_docs=65000)"
   ],
   "metadata": {
    "id": "lcW3EJplR4Vy",
    "ExecuteTime": {
     "end_time": "2025-12-16T14:28:42.056124Z",
     "start_time": "2025-12-16T14:13:17.289809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_17308\\132209790.py:20: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  self.evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
      "C:\\Users\\marci\\AppData\\Local\\Temp\\ipykernel_17308\\132209790.py:21: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  self.evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Vectorizing corpus from ragqa_arena_tech_corpus.jsonl...\n",
      "❓ Loading QA pairs from ragqa_arena_tech_examples.jsonl...\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "id": "5c1d5eb8",
    "outputId": "a5666006-28b2-49dc-a829-38062a11fe16",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-12-16T14:36:19.259439Z",
     "start_time": "2025-12-16T14:36:19.183083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the total number of documents in the collection\n",
    "total_docs = runner.collection.count()\n",
    "print(total_docs)\n",
    "\n",
    "if total_docs == 0:\n",
    "    print(\"No documents in the collection.\")\n",
    "else:\n",
    "    # Determine how many documents to show (max 5 or total_docs if less than 5)\n",
    "    num_to_show = min(5, total_docs)\n",
    "\n",
    "    # Generate random indices from the range of existing document IDs\n",
    "    random_indices = np.random.choice(total_docs, num_to_show, replace=False)\n",
    "\n",
    "\n",
    "    # Construct the document IDs based on how they were inserted (e.g., 'doc_0', 'doc_1', etc.)\n",
    "    random_ids = [f\"{i}\" for i in random_indices]\n",
    "    print(random_ids)\n",
    "\n",
    "    # Retrieve the selected documents from ChromaDB\n",
    "    random_documents = runner.collection.get(ids=random_ids)\n",
    "    print(random_documents)\n",
    "    print(f\"Displaying {num_to_show} random documents from the collection:\")\n",
    "    for i, doc_content in enumerate(random_documents['documents']):\n",
    "        print(f\"\\n--- Document {i+1} (ID: {random_documents['ids'][i]}) ---\")\n",
    "        print(doc_content)\n",
    "        print(\"-\" * 30)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28436\n",
      "['11424', '23496', '20327', '247', '11142']\n",
      "{'ids': ['247', '11142', '11424', '20327', '23496'], 'embeddings': None, 'documents': ['Also available are iosnoop and iotop depending on your specific needs. These terminal commands can be piped through grep to watch for filesystem events from a specific process or against a specific file.', 'If you have no other Apple devices, then when you chose to delete your passwords, you likely deleted them from your phone (at first). But when you re-enabled iCloud, you synced your current phone status to iCloud, which is / was password-less and removed the content from iCloud as well.', 'A static IP for at least one side is advised; however, DDNS will work for this,(if both sides are assigned dynamic addresses and NAT Overloaded), while both routers have fqdns assigned for dynamic tracking of peer: http://www.cisco.com/c/en/us/support/docs/security-vpn/ipsec-architecture-implementation/118048-technote-ipsec-00.html . If DDNS is not leveraged, one side will require to have a public static IP in order to provide a peer IP Address for the remote dynamic peer to initiate interesting traffic. I recommend strongly in getting real time support from Cisco TAC on this configuration and the requirements.', 'If you have one of the supported phone models: Theres an app called voodoo sound. Voodoo Sound in Googles Play store Downside: Requires a custom kernel, only for supported phones (see description in market) The paid app comes without a requirement for a custom kernel (but needs root) Voodoo Sound Plus Both replace the audio codecs driver and allow fine tuning and to turn on formerly unused chip features (see description).', 'Its not a hardware limitation if you have Bluetooth 4.0, you can see it in apple>About this Mac > System Information > bluetooth > LMP Version They just used the easy way to enable or disable the feature if model greater than 2012 its enabled otherwise not, despite having the correct hardware.'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'source': 'ragqa'}, {'source': 'ragqa'}, {'source': 'ragqa'}, {'source': 'ragqa'}, {'source': 'ragqa'}]}\n",
      "Displaying 5 random documents from the collection:\n",
      "\n",
      "--- Document 1 (ID: 247) ---\n",
      "Also available are iosnoop and iotop depending on your specific needs. These terminal commands can be piped through grep to watch for filesystem events from a specific process or against a specific file.\n",
      "------------------------------\n",
      "\n",
      "--- Document 2 (ID: 11142) ---\n",
      "If you have no other Apple devices, then when you chose to delete your passwords, you likely deleted them from your phone (at first). But when you re-enabled iCloud, you synced your current phone status to iCloud, which is / was password-less and removed the content from iCloud as well.\n",
      "------------------------------\n",
      "\n",
      "--- Document 3 (ID: 11424) ---\n",
      "A static IP for at least one side is advised; however, DDNS will work for this,(if both sides are assigned dynamic addresses and NAT Overloaded), while both routers have fqdns assigned for dynamic tracking of peer: http://www.cisco.com/c/en/us/support/docs/security-vpn/ipsec-architecture-implementation/118048-technote-ipsec-00.html . If DDNS is not leveraged, one side will require to have a public static IP in order to provide a peer IP Address for the remote dynamic peer to initiate interesting traffic. I recommend strongly in getting real time support from Cisco TAC on this configuration and the requirements.\n",
      "------------------------------\n",
      "\n",
      "--- Document 4 (ID: 20327) ---\n",
      "If you have one of the supported phone models: Theres an app called voodoo sound. Voodoo Sound in Googles Play store Downside: Requires a custom kernel, only for supported phones (see description in market) The paid app comes without a requirement for a custom kernel (but needs root) Voodoo Sound Plus Both replace the audio codecs driver and allow fine tuning and to turn on formerly unused chip features (see description).\n",
      "------------------------------\n",
      "\n",
      "--- Document 5 (ID: 23496) ---\n",
      "Its not a hardware limitation if you have Bluetooth 4.0, you can see it in apple>About this Mac > System Information > bluetooth > LMP Version They just used the easy way to enable or disable the feature if model greater than 2012 its enabled otherwise not, despite having the correct hardware.\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T15:06:34.765050Z",
     "start_time": "2025-12-16T15:06:32.660978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Your existing in-memory client (with data)\n",
    "in_memory_col = runner.collection\n",
    "\n",
    "# 2. Initialize a NEW persistent client\n",
    "persistent_client = chromadb.PersistentClient(path=\"./demodata\")\n",
    "persistent_col = persistent_client.get_or_create_collection(\"rag_qa_arena_tech_v3\")\n",
    "\n",
    "# 3. Retrieve all data from memory\n",
    "# Get all data (ids, embeddings, documents, metadatas)\n",
    "existing_data = in_memory_col.get(include=['embeddings', 'documents', 'metadatas'])"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T14:48:58.516091Z",
     "start_time": "2025-12-16T14:48:58.503593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_in_batches(\n",
    "    col,\n",
    "    ids,\n",
    "    embeddings=None,\n",
    "    documents=None,\n",
    "    metadatas=None,\n",
    "    batch_size=5000,  # choose <= your observed max (5461). 5000 gives headroom.\n",
    "):\n",
    "    if not ids:\n",
    "        return\n",
    "\n",
    "    n = len(ids)\n",
    "\n",
    "    # Basic alignment checks (fail fast, prevents subtle corruption)\n",
    "    if embeddings is not None and len(embeddings) != n:\n",
    "        raise ValueError(f\"len(embeddings)={len(embeddings)} must match len(ids)={n}\")\n",
    "    if documents is not None and len(documents) != n:\n",
    "        raise ValueError(f\"len(documents)={len(documents)} must match len(ids)={n}\")\n",
    "    if metadatas is not None and len(metadatas) != n:\n",
    "        raise ValueError(f\"len(metadatas)={len(metadatas)} must match len(ids)={n}\")\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "\n",
    "        kwargs = {\"ids\": ids[start:end]}\n",
    "        if embeddings is not None:\n",
    "            kwargs[\"embeddings\"] = embeddings[start:end]\n",
    "        if documents is not None:\n",
    "            kwargs[\"documents\"] = documents[start:end]\n",
    "        if metadatas is not None:\n",
    "            kwargs[\"metadatas\"] = metadatas[start:end]\n",
    "\n",
    "        col.add(**kwargs)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T14:50:22.489161Z",
     "start_time": "2025-12-16T14:49:22.323740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_in_batches(\n",
    "    persistent_col,\n",
    "    ids=existing_data[\"ids\"],\n",
    "    embeddings=existing_data.get(\"embeddings\"),\n",
    "    documents=existing_data.get(\"documents\"),\n",
    "    metadatas=existing_data.get(\"metadatas\"),\n",
    "    batch_size=5000,  # <= 5461\n",
    ")\n",
    "print(\"Data successfully migrated to disk!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully migrated to disk!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T14:50:52.975828Z",
     "start_time": "2025-12-16T14:50:52.966585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split for Train (Optimization) and Test (Evaluation)\n",
    "train_data = all_data[:5]\n",
    "test_data = all_data[5:]\n",
    "\n",
    "# 2. Instantiate the uncompiled DSPy Module\n",
    "rag_module = DSPyRAGModule(retrieve_fn=runner.retrieve)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T14:50:57.029903Z",
     "start_time": "2025-12-16T14:50:57.021360Z"
    }
   },
   "cell_type": "code",
   "source": "len(all_data)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T19:29:32.706360Z",
     "start_time": "2025-12-16T19:26:24.322939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ragas import evaluate\n",
    "runner.evaluate_system(rag_module, test_data, name=\"Baseline\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Evaluating Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [03:00<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Baseline Results: {'faithfulness': 0.9603, 'answer_correctness': 0.6147, 'context_precision': 0.8333, 'answer_relevancy': 0.9558}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'faithfulness': 0.9603, 'answer_correctness': 0.6147, 'context_precision': 0.8333, 'answer_relevancy': 0.9558},\n",
       " Dataset({\n",
       "     features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "     num_rows: 5\n",
       " }))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###DSPY Optimization"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T15:57:10.652403Z",
     "start_time": "2025-12-16T15:57:07.346347Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'ragqa_arena_tech_examples.jsonl'...\n"
     ]
    }
   ],
   "execution_count": 44,
   "source": [
    "import ujson\n",
    "from dspy.utils import download\n",
    "\n",
    "# Download question--answer pairs from the RAG-QA Arena \"Tech\" dataset.\n",
    "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\")\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [ujson.loads(line) for line in f]"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T15:57:38.742196Z",
     "start_time": "2025-12-16T15:57:38.734219Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'why igp is used in mpls?',\n",
       " 'response': \"An IGP exchanges routing prefixes between gateways/routers.  \\nWithout a routing protocol, you'd have to configure each route on every router and you'd have no dynamic updates when routes change because of link failures. \\nFuthermore, within an MPLS network, an IGP is vital for advertising the internal topology and ensuring connectivity for MP-BGP inside the network.\",\n",
       " 'gold_doc_ids': [2822, 2823]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45,
   "source": [
    "# Inspect one datapoint.\n",
    "data[0]"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:05:27.454936Z",
     "start_time": "2025-12-16T16:05:27.415615Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'what is b in grep?', 'response': 'In regular expressions, the term \"\\\\b\" signifies a \"word boundary,\" and the command searches for all words i in the file linux.txt.', 'gold_doc_ids': [4783]}) (input_keys={'question'})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54,
   "source": [
    "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
    "\n",
    "# Let's pick an `example` here from the data.\n",
    "example = data[2]\n",
    "example"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T15:58:37.986320Z",
     "start_time": "2025-12-16T15:58:37.975312Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47,
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:02:05.733045Z",
     "start_time": "2025-12-16T16:02:05.724482Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 49,
   "source": [
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:02:54.877103Z",
     "start_time": "2025-12-16T16:02:44.451792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qa = dspy.Predict('question: str -> response: str')\n",
    "response = qa(question=\"what are high memory and low memory on linux?\")\n",
    "\n",
    "print(response.response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Linux, \"high memory\" and \"low memory\" refer to different ranges of memory addresses that the kernel can utilize, particularly in 32-bit systems:\n",
      "\n",
      "1. **Low Memory**: This represents the lower portion of the memory range, typically below 4 GB (or 1 GB in some configurations). This memory is directly accessible by the kernel and can be used for various tasks, such as kernel data structures and process management. Low memory is preferable for operations that require quicker access because it doesn't involve any additional handling.\n",
      "\n",
      "2. **High Memory**: This refers to memory addresses that are above the low memory address range (above 4 GB on a 32-bit system). The kernel cannot directly access this memory. Instead, it requires special mechanisms like paging to access high memory. This is relevant for systems with large amounts of RAM when they utilize a 64-bit architecture, allowing for more efficient memory use. High memory is often used for user-space processes or data that doesn't require constant kernel access.\n",
      "\n",
      "In summary, low memory is directly usable by the kernel, while high memory needs special handling due to the addressable space limits in 32-bit systems. In 64-bit systems, the distinction is less critical as the kernel can address a much larger memory space directly.\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:03:22.076207Z",
     "start_time": "2025-12-16T16:03:22.069565Z"
    }
   },
   "cell_type": "code",
   "source": "dspy.inspect_history(n=1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-12-16T17:02:54.873275]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str):\n",
      "Your output fields are:\n",
      "1. `response` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "what are high memory and low memory on linux?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m[[ ## response ## ]]\n",
      "In Linux, \"high memory\" and \"low memory\" refer to different ranges of memory addresses that the kernel can utilize, particularly in 32-bit systems:\n",
      "\n",
      "1. **Low Memory**: This represents the lower portion of the memory range, typically below 4 GB (or 1 GB in some configurations). This memory is directly accessible by the kernel and can be used for various tasks, such as kernel data structures and process management. Low memory is preferable for operations that require quicker access because it doesn't involve any additional handling.\n",
      "\n",
      "2. **High Memory**: This refers to memory addresses that are above the low memory address range (above 4 GB on a 32-bit system). The kernel cannot directly access this memory. Instead, it requires special mechanisms like paging to access high memory. This is relevant for systems with large amounts of RAM when they utilize a 64-bit architecture, allowing for more efficient memory use. High memory is often used for user-space processes or data that doesn't require constant kernel access.\n",
      "\n",
      "In summary, low memory is directly usable by the kernel, while high memory needs special handling due to the addressable space limits in 32-bit systems. In 64-bit systems, the distinction is less critical as the kernel can address a much larger memory space directly.\n",
      "\n",
      "[[ ## completed ## ]]\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:04:17.085343Z",
     "start_time": "2025-12-16T16:04:12.892868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cot = dspy.ChainOfThought('question -> response')\n",
    "cot(question=\"should curly braces appear on their own line?\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning=\"The placement of curly braces largely depends on the style guide or coding standard you are following. In many programming languages, particularly those influenced by C-style syntax (like Java, C++, and JavaScript), there's a common practice of placing opening curly braces on the same line as the statement (like for loops or function definitions) and placing closing braces on their own line. This keeps related code visually grouped together. However, some coding styles, like the one advocated by Python (which uses indentation instead of braces), or other specific style guides, might suggest different practices. Ultimately, it's about consistency and adhering to the conventions established for the project or language you are working with.\",\n",
       "    response='Curly braces do not necessarily have to appear on their own line; it depends on the coding style guide you are following. Many prefer placing the opening brace on the same line as the preceding statement while placing the closing brace on its own line for clarity. Consistency with the chosen style is key.'\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:10:34.716069Z",
     "start_time": "2025-12-16T16:10:34.703796Z"
    }
   },
   "cell_type": "code",
   "source": "data[2]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'what is b in grep?', 'response': 'In regular expressions, the term \"\\\\b\" signifies a \"word boundary,\" and the command searches for all words i in the file linux.txt.', 'gold_doc_ids': [4783]}) (input_keys={'question'})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:11:56.818884Z",
     "start_time": "2025-12-16T16:11:44.772865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "example = data[3]\n",
    "\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "pred = cot(**example.inputs())\n",
    "#print(pred)\n",
    "# Compute the metric score for the prediction.\n",
    "score = metric(example, pred)\n",
    "\n",
    "print(f\"Question: \\t {example.question}\\n\")\n",
    "print(f\"Gold Response: \\t {example.response}\\n\")\n",
    "print(f\"Predicted Response: \\t {pred.response}\\n\")\n",
    "print(f\"Semantic F1 Score: {score:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \t how can i recursively delete empty directories in my home directory?\n",
      "\n",
      "Gold Response: \t To remove directories using command-line operations, it is possible to call rmdir on each directory, as it will only delete directories that are empty.\n",
      "\n",
      "Predicted Response: \t You can use the following command in your terminal to recursively delete empty directories in your home directory:\n",
      "\n",
      "```bash\n",
      "find ~/ -type d -empty -delete\n",
      "```\n",
      "\n",
      "This command will search for all empty directories within your home directory and delete them.\n",
      "\n",
      "Semantic F1 Score: 0.50\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:12:59.399716Z",
     "start_time": "2025-12-16T16:12:59.390273Z"
    }
   },
   "cell_type": "code",
   "source": "dspy.inspect_history(n=2)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-12-16T17:11:49.636668]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `response` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "how can i recursively delete empty directories in my home directory?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m[[ ## reasoning ## ]]\n",
      "To recursively delete empty directories in your home directory, you can use the command line. The `find` command is very useful for this task. The command `find ~/ -type d -empty -delete` will search for all empty directories starting from your home directory (`~/`) and delete them. The `-type d` option tells `find` to look for directories, while `-empty` filters out only those that are empty. The `-delete` at the end will remove the empty directories found.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "You can use the following command in your terminal to recursively delete empty directories in your home directory:\n",
      "\n",
      "```bash\n",
      "find ~/ -type d -empty -delete\n",
      "```\n",
      "\n",
      "This command will search for all empty directories within your home directory and delete them.\n",
      "\n",
      "[[ ## completed ## ]]\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-12-16T17:11:56.812921]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str): \n",
      "2. `ground_truth` (str): \n",
      "3. `system_response` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `ground_truth_key_ideas` (str): enumeration of key ideas in the ground truth\n",
      "3. `system_response_key_ideas` (str): enumeration of key ideas in the system response\n",
      "4. `discussion` (str): discussion of the overlap between ground truth and system response\n",
      "5. `recall` (float): fraction (out of 1.0) of ground truth covered by the system response\n",
      "6. `precision` (float): fraction (out of 1.0) of system response covered by the ground truth\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## ground_truth ## ]]\n",
      "{ground_truth}\n",
      "\n",
      "[[ ## system_response ## ]]\n",
      "{system_response}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## ground_truth_key_ideas ## ]]\n",
      "{ground_truth_key_ideas}\n",
      "\n",
      "[[ ## system_response_key_ideas ## ]]\n",
      "{system_response_key_ideas}\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "{discussion}\n",
      "\n",
      "[[ ## recall ## ]]\n",
      "{recall}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## precision ## ]]\n",
      "{precision}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
      "        You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "how can i recursively delete empty directories in my home directory?\n",
      "\n",
      "[[ ## ground_truth ## ]]\n",
      "To remove directories using command-line operations, it is possible to call rmdir on each directory, as it will only delete directories that are empty.\n",
      "\n",
      "[[ ## system_response ## ]]\n",
      "You can use the following command in your terminal to recursively delete empty directories in your home directory:\n",
      "\n",
      "```bash\n",
      "find ~/ -type d -empty -delete\n",
      "```\n",
      "\n",
      "This command will search for all empty directories within your home directory and delete them.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## ground_truth_key_ideas ## ]]`, then `[[ ## system_response_key_ideas ## ]]`, then `[[ ## discussion ## ]]`, then `[[ ## recall ## ]]` (must be formatted as a valid Python float), then `[[ ## precision ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m[[ ## reasoning ## ]]\n",
      "The ground truth discusses the use of the `rmdir` command for deleting empty directories, while the system response provides a specific command using `find` to achieve the same goal. Although both pieces of information pertain to deleting empty directories, the system response offers a more advanced method that can delete multiple empty directories recursively, which is not mentioned in the ground truth. \n",
      "\n",
      "[[ ## ground_truth_key_ideas ## ]]\n",
      "1. Use the command `rmdir` to delete empty directories.\n",
      "2. `rmdir` only deletes directories that are empty.\n",
      "\n",
      "[[ ## system_response_key_ideas ## ]]\n",
      "1. Command to recursively delete empty directories: `find ~/ -type d -empty -delete`.\n",
      "2. The command searches for all empty directories and deletes them.\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "The key ideas in the ground truth focus on using `rmdir` for deleting empty directories, while the system response presents a more efficient command using `find` that allows for searching and deleting multiple empty directories at once. The system response does not directly reference `rmdir`, which may decrease its relevance to the ground truth. However, both responses address the overarching goal of deleting empty directories.\n",
      "\n",
      "[[ ## recall ## ]]\n",
      "0.5  # The system response covers the idea of deleting empty directories, but does not mention `rmdir`.\n",
      "\n",
      "[[ ## precision ## ]]\n",
      "0.5  # The ground truth only pertains to the method of deleting empty directories without the advanced technique presented in the system response.\n",
      "\n",
      "[[ ## completed ## ]]\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:17:04.480065Z",
     "start_time": "2025-12-16T16:13:31.372817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define an evaluator that we can re-use.\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
    "                         display_progress=True, display_table=2)\n",
    "\n",
    "# Evaluate the Chain-of-Thought program.\n",
    "evaluate(cot)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 127.19 / 300 (42.4%): 100%|██████████| 300/300 [03:32<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:17:04 INFO dspy.evaluate.evaluate: Average Metric: 127.19304820331601 / 300 (42.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                       question  \\\n",
       "0       when to use c over c++, and c++ over c?   \n",
       "1  should images be stored in a git repository?   \n",
       "\n",
       "                                                        example_response  \\\n",
       "0  If you are equally familiar with both C++ and C, it's advisable to...   \n",
       "1  One viewpoint expresses that there is no significant downside, esp...   \n",
       "\n",
       "                     gold_doc_ids  \\\n",
       "0                           [733]   \n",
       "1  [6253, 6254, 6275, 6278, 8215]   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  C and C++ are both powerful programming languages, but they serve ...   \n",
       "1  Storing images in a Git repository can lead to several challenges....   \n",
       "\n",
       "                                                           pred_response  \\\n",
       "0  Use C when you need low-level programming with high performance an...   \n",
       "1  No, images should generally not be stored in a Git repository due ...   \n",
       "\n",
       "   SemanticF1  \n",
       "0  ✔️ [0.489]  \n",
       "1  ✔️ [0.240]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>example_response</th>\n",
       "      <th>gold_doc_ids</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_response</th>\n",
       "      <th>SemanticF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when to use c over c++, and c++ over c?</td>\n",
       "      <td>If you are equally familiar with both C++ and C, it's advisable to...</td>\n",
       "      <td>[733]</td>\n",
       "      <td>C and C++ are both powerful programming languages, but they serve ...</td>\n",
       "      <td>Use C when you need low-level programming with high performance an...</td>\n",
       "      <td>✔️ [0.489]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>should images be stored in a git repository?</td>\n",
       "      <td>One viewpoint expresses that there is no significant downside, esp...</td>\n",
       "      <td>[6253, 6254, 6275, 6278, 8215]</td>\n",
       "      <td>Storing images in a Git repository can lead to several challenges....</td>\n",
       "      <td>No, images should generally not be stored in a Git repository due ...</td>\n",
       "      <td>✔️ [0.240]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 298 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(score=42.4, results=<list of 300 results>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:18:10.746935Z",
     "start_time": "2025-12-16T16:18:07.636122Z"
    }
   },
   "cell_type": "code",
   "source": "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\")",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:25:52.094064Z",
     "start_time": "2025-12-16T16:20:36.402501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [ujson.loads(line)['text'][:max_characters] for line in f]\n",
    "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
    "\n",
    "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28436 documents. Will encode them below.\n",
      "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:25:52.220720Z",
     "start_time": "2025-12-16T16:25:52.210338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RAGModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:26:00.659851Z",
     "start_time": "2025-12-16T16:25:52.274847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag = RAGModule()\n",
    "rag(question=\"what are high memory and low memory on linux?\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='High memory and low memory in Linux refer to different segments of the memory addressing space utilized by the kernel and user-space applications. Low memory is the region of memory that is always mapped in the kernel’s address space, allowing the kernel direct access for its operations. This memory is used for critical kernel data structures. Conversely, high memory refers to the portion of system memory that is not permanently mapped to the kernel’s address space. When the kernel needs to access high memory, it must temporarily map it into its address space. This distinction is crucial in managing memory, especially in a 32-bit architecture where the kernel needs to efficiently handle memory allocation and access for both kernel and user processes.',\n",
       "    response='In Linux, high memory is the segment of memory that is not permanently mapped into the kernel’s address space and requires special procedures for access. Low memory, on the other hand, is always mapped and is used directly by the kernel for managing its data structures. Typically in a 32-bit Linux system, low memory covers the first portion of the memory space, allowing for efficient access by the kernel, while high memory includes the segments above that which the kernel must map in only when needed.'\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:26:40.132295Z",
     "start_time": "2025-12-16T16:26:40.124117Z"
    }
   },
   "cell_type": "code",
   "source": "dspy.inspect_history()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-12-16T17:26:00.653378]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): \n",
      "2. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `response` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «As far as I remember, High Memory is used for application space and Low Memory for the kernel. Advantage is that (user-space) applications cant access kernel-space memory.»\n",
      "[2] «HIGHMEM is a range of kernels memory space, but it is NOT memory you access but its a place where you put what you want to access. A typical 32bit Linux virtual memory map is like: 0x00000000-0xbfffffff: user process (3GB) 0xc0000000-0xffffffff: kernel space (1GB) (CPU-specific vector and whatsoever are ignored here). Linux splits the 1GB kernel space into 2 pieces, LOWMEM and HIGHMEM. The split varies from installation to installation. If an installation chooses, say, 512MB-512MB for LOW and HIGH mems, the 512MB LOWMEM (0xc0000000-0xdfffffff) is statically mapped at the kernel boot time; usually the first so many bytes of the physical memory is used for this so that virtual and physical addresses in this range have a constant offset of, say, 0xc0000000. On the other hand, the latter 512MB (HIGHMEM) has no static mapping (although you could leave pages semi-permanently mapped there, but you must do so explicitly in your driver code). Instead, pages are temporarily mapped and unmapped here so that virtual and physical addresses in this range have no consistent mapping. Typical uses of HIGHMEM include single-time data buffers.»\n",
      "[3] «This is relevant to the Linux kernel; Im not sure how any Unix kernel handles this. The High Memory is the segment of memory that user-space programs can address. It cannot touch Low Memory. Low Memory is the segment of memory that the Linux kernel can address directly. If the kernel must access High Memory, it has to map it into its own address space first. There was a patch introduced recently that lets you control where the segment is. The tradeoff is that you can take addressable memory away from user space so that the kernel can have more memory that it does not have to map before using. Additional resources: http://tldp.org/HOWTO/KernelAnalysis-HOWTO-7.html http://linux-mm.org/HighMemory»\n",
      "[4] «The first reference to turn to is Linux Device Drivers (available both online and in book form), particularly chapter 15 which has a section on the topic. In an ideal world, every system component would be able to map all the memory it ever needs to access. And this is the case for processes on Linux and most operating systems: a 32-bit process can only access a little less than 2^32 bytes of virtual memory (in fact about 3GB on a typical Linux 32-bit architecture). It gets difficult for the kernel, which needs to be able to map the full memory of the process whose system call its executing, plus the whole physical memory, plus any other memory-mapped hardware device. So when a 32-bit kernel needs to map more than 4GB of memory, it must be compiled with high memory support. High memory is memory which is not permanently mapped in the kernels address space. (Low memory is the opposite: it is always mapped, so you can access it in the kernel simply by dereferencing a pointer.) When you access high memory from kernel code, you need to call kmap first, to obtain a pointer from a page data structure (struct page). Calling kmap works whether the page is in high or low memory. There is also kmap_atomic which has added constraints but is more efficient on multiprocessor machines because it uses finer-grained locking. The pointer obtained through kmap is a resource: it uses up address space. Once youve finished with it, you must call kunmap (or kunmap_atomic) to free that resource; then the pointer is no longer valid, and the contents of the page cant be accessed until you call kmap again.»\n",
      "[5] «/proc/meminfo will tell you how free works, but /proc/kcore can tell you what the kernel uses. From the same page: /proc/kcore This file represents the physical memory of the system and is stored in the ELF core file format. With this pseudo-file, and an unstripped kernel (/usr/src/linux/vmlinux) binary, GDB can be used to examine the current state of any kernel data structures. The total length of the file is the size of physical memory (RAM) plus 4KB. /proc/meminfo This file reports statistics about memory usage on the system. It is used by free(1) to report the amount of free and used memory (both physical and swap) on the system as well as the shared memory and buffers used by the kernel. Each line of the file consists of a parameter name, followed by a colon, the value of the parameter, and an option unit of measurement (e.g., kB). The list below describes the parameter names and the format specifier required to read the field value. Except as noted below, all of the fields have been present since at least Linux 2.6.0. Some fileds are displayed only if the kernel was configured with various options; those dependencies are noted in the list. MemTotal %lu Total usable RAM (i.e., physical RAM minus a few reserved bits and the kernel binary code). MemFree %lu The sum of LowFree+HighFree. Buffers %lu Relatively temporary storage for raw disk blocks that shouldnt get tremendously large (20MB or so). Cached %lu In-memory cache for files read from the disk (the page cache). Doesnt include SwapCached. SwapCached %lu Memory that once was swapped out, is swapped back in but still also is in the swap file. (If memory pressure is high, these pages dont need to be swapped out again because they are already in the swap file. This saves I/O.) Active %lu Memory that has been used more recently and usually not reclaimed unless absolutely necessary. Inactive %lu Memory which has been less recently used. It is more eligible to be reclaimed for other purposes. Active(anon) %lu (since Linux 2.6.28) [To be documented.] Inactive(anon) %lu (since Linux 2.6.28) [To be documented.] Active(file) %lu (since Linux 2.6.28) [To be documented.] Inactive(file) %lu (since Linux 2.6.28) [To be documented.] Unevictable %lu (since Linux 2.6.28) (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.) [To be documented.] Mlocked %lu (since Linux 2.6.28) (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.) [To be documented.] HighTotal %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Total amount of highmem. Highmem is all memory above ~860MB of physical memory. Highmem areas are for use by user-space programs, or for the page cache. The kernel must use tricks to access this memory, making it slower to access than lowmem. HighFree %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Amount of free highmem. LowTotal %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Total amount of lowmem. Lowmem is memory which can be used for everything that highmem can be used for, but it is also available for the kernels use for its own data structures. Among many other things, it is where everything from Slab is allocated. Bad things happen when youre out of lowmem. LowFree %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Amount of free lowmem. MmapCopy %lu (since Linux 2.6.29) (CONFIG_MMU is required.) [To be documented.] SwapTotal %lu Total amount of swap space available. SwapFree %lu Amount of swap space that is currently unused. Dirty %lu Memory which is waiting to get written back to the disk. Writeback %lu Memory which is actively being written back to the disk. AnonPages %lu (since Linux 2.6.18) Non-file backed pages mapped into user-space page tables. Mapped %lu Files which have been mmaped, such as libraries. Shmem %lu (since Linux 2.6.32) [To be documented.] Slab %lu In-kernel data structures cache. SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches. SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. KernelStack %lu (since Linux 2.6.32) Amount of memory allocated to kernel stacks. PageTables %lu (since Linux 2.6.18) Amount of memory dedicated to the lowest level of page tables. Quicklists %lu (since Linux 2.6.27) (CONFIG_QUICKLIST is required.) [To be documented.] NFS_Unstable %lu (since Linux 2.6.18) NFS pages sent to the server, but not yet committed to stable storage. Bounce %lu (since Linux 2.6.18) Memory used for block device bounce buffers. WritebackTmp %lu (since Linux 2.6.26) Memory used by FUSE for temporary writeback buffers. CommitLimit %lu (since Linux 2.6.10) Based on the overcommit ratio (vm.overcommit_ratio), this is the total amount of memory currently available to be allocated on the system. This limit is adhered to only if strict overcommit accounting is enabled (mode 2 in /proc/sys/vm/overcommit_ratio). The CommitLimit is calculated using the following formula: CommitLimit = ([total RAM pages] - [total huge TLB pages]) * overcommit_ratio / 100 + [total swap pages] For example, on a system with 1GB of physical RAM and 7GB of swap with a overcommit_ratio of 30, this formula yields a CommitLimit of 7.3GB. For more details, see the memory overcommit documentation in the kernel source file Documentation/vm/overcommit-accounting. Committed_AS %lu The amount of memory presently allocated on the system. The committed memory is a sum of all of the memory which has been allocated by processes, even if it has not been used by them as of yet. A process which allocates 1GB of memory (using malloc(3) or similar), but touches only 300MB of that memory will show up as using only 300MB of memory even if it has the address space allocated for the entire 1GB. This 1GB is memory which has been committed to by the VM and can be used at any time by the allocating application. With strict overcommit enabled on the system (mode 2 /proc/sys/vm/overcommit_memory), allocations w»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "what are high memory and low memory on linux?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m[[ ## reasoning ## ]]\n",
      "High memory and low memory in Linux refer to different segments of the memory addressing space utilized by the kernel and user-space applications. Low memory is the region of memory that is always mapped in the kernel’s address space, allowing the kernel direct access for its operations. This memory is used for critical kernel data structures. Conversely, high memory refers to the portion of system memory that is not permanently mapped to the kernel’s address space. When the kernel needs to access high memory, it must temporarily map it into its address space. This distinction is crucial in managing memory, especially in a 32-bit architecture where the kernel needs to efficiently handle memory allocation and access for both kernel and user processes.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "In Linux, high memory is the segment of memory that is not permanently mapped into the kernel’s address space and requires special procedures for access. Low memory, on the other hand, is always mapped and is used directly by the kernel for managing its data structures. Typically in a 32-bit Linux system, low memory covers the first portion of the memory space, allowing for efficient access by the kernel, while high memory includes the segments above that which the kernel must map in only when needed.\n",
      "\n",
      "[[ ## completed ## ]]\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T16:31:21.077689Z",
     "start_time": "2025-12-16T16:27:32.440290Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate(RAGModule())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 164.33 / 300 (54.8%): 100%|██████████| 300/300 [03:48<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:31:21 INFO dspy.evaluate.evaluate: Average Metric: 164.32849042819623 / 300 (54.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                       question  \\\n",
       "0       when to use c over c++, and c++ over c?   \n",
       "1  should images be stored in a git repository?   \n",
       "\n",
       "                                                        example_response  \\\n",
       "0  If you are equally familiar with both C++ and C, it's advisable to...   \n",
       "1  One viewpoint expresses that there is no significant downside, esp...   \n",
       "\n",
       "                     gold_doc_ids  \\\n",
       "0                           [733]   \n",
       "1  [6253, 6254, 6275, 6278, 8215]   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  C should be preferred over C++ when developing for embedded system...   \n",
       "1  Storing images in a Git repository is generally not recommended du...   \n",
       "\n",
       "                                                           pred_response  \\\n",
       "0  Use C over C++ when developing embedded systems or applications wh...   \n",
       "1  In summary, while it is possible to store images in a Git reposito...   \n",
       "\n",
       "   SemanticF1  \n",
       "0  ✔️ [0.500]  \n",
       "1  ✔️ [0.429]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>example_response</th>\n",
       "      <th>gold_doc_ids</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_response</th>\n",
       "      <th>SemanticF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when to use c over c++, and c++ over c?</td>\n",
       "      <td>If you are equally familiar with both C++ and C, it's advisable to...</td>\n",
       "      <td>[733]</td>\n",
       "      <td>C should be preferred over C++ when developing for embedded system...</td>\n",
       "      <td>Use C over C++ when developing embedded systems or applications wh...</td>\n",
       "      <td>✔️ [0.500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>should images be stored in a git repository?</td>\n",
       "      <td>One viewpoint expresses that there is no significant downside, esp...</td>\n",
       "      <td>[6253, 6254, 6275, 6278, 8215]</td>\n",
       "      <td>Storing images in a Git repository is generally not recommended du...</td>\n",
       "      <td>In summary, while it is possible to store images in a Git reposito...</td>\n",
       "      <td>✔️ [0.429]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 298 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(score=54.78, results=<list of 300 results>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T17:07:34.454788Z",
     "start_time": "2025-12-16T16:37:17.798721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tp = dspy.MIPROv2(metric=metric, auto=\"medium\", num_threads=24)  # use fewer threads if your rate limit is small\n",
    "\n",
    "optimized_rag = tp.compile(RAGModule(), trainset=trainset,\n",
    "                           max_bootstrapped_demos=2, max_labeled_demos=2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:37:17 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING MEDIUM AUTO RUN SETTINGS:\n",
      "num_trials: 18\n",
      "minibatch: True\n",
      "num_fewshot_candidates: 12\n",
      "num_instruct_candidates: 6\n",
      "valset size: 160\n",
      "\n",
      "2025/12/16 17:37:17 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/12/16 17:37:17 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/12/16 17:37:17 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/12\n",
      "Bootstrapping set 2/12\n",
      "Bootstrapping set 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [01:25<10:00, 17.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Bootstrapping set 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [01:45<09:59, 17.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n",
      "Bootstrapping set 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:19<12:22, 19.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:54<11:08, 18.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:34<11:04, 17.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [01:02<09:24, 15.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Bootstrapping set 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:34<10:54, 17.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Bootstrapping set 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [01:21<09:33, 16.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Bootstrapping set 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [01:10<08:15, 14.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Bootstrapping set 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:11<07:39, 11.79s/it]\n",
      "2025/12/16 17:46:39 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/12/16 17:46:39 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:46:59 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=6 instructions...\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: 1: Imagine you are a tech support specialist assisting a user with a critical technical issue on a macOS or Unix/Linux system. Your task is to provide a comprehensive explanation to their query based on the provided context. Using the fields `context` and `question`, think critically about the information, and produce a detailed `response` that not only answers the user's question but also includes practical advice or troubleshooting tips they can apply immediately. Ensure your response empowers the user to understand their situation better and take the necessary steps to resolve their issue.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Create a comprehensive response that integrates the provided `context` with the `question`, ensuring to highlight key methods and details relevant to the inquiry. Clearly explain any necessary steps or commands needed to achieve the solution, while considering different user preferences or scenarios that may arise in practical usage.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Using the provided `context` and the `question`, generate a thorough yet succinct `response` that encompasses multiple methods or solutions relevant to the query while ensuring clarity and user-friendliness.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: 4: You are a technical guide familiar with troubleshooting macOS and Unix/Linux systems. Given the fields `context` (background information) and `question` (user inquiry), produce a detailed `response` that includes clear explanations and examples related to best practices in command-line usage and system management. Ensure that your response enhances understanding and user confidence in managing technical challenges.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: 5: Given the `context` information about macOS and Unix/Linux troubleshooting, along with the `question` being asked, provide a detailed `reasoning` process and a comprehensive `response` that addresses the question clearly and effectively, ensuring the explanation is tailored to various levels of user expertise.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/12/16 17:48:03 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 23 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 89.97 / 160 (56.2%): 100%|██████████| 160/160 [01:53<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:49:57 INFO dspy.evaluate.evaluate: Average Metric: 89.96909671730273 / 160 (56.2%)\n",
      "2025/12/16 17:49:57 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 56.23\n",
      "\n",
      "C:\\Users\\marci\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\optuna\\_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/12/16 17:49:57 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 22.56 / 35 (64.4%): 100%|██████████| 35/35 [00:42<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:50:39 INFO dspy.evaluate.evaluate: Average Metric: 22.556492536651767 / 35 (64.4%)\n",
      "2025/12/16 17:50:39 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 64.45 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 17:50:39 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45]\n",
      "2025/12/16 17:50:39 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23]\n",
      "2025/12/16 17:50:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 56.23\n",
      "2025/12/16 17:50:39 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:50:39 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.87 / 35 (56.8%): 100%|██████████| 35/35 [00:40<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:51:19 INFO dspy.evaluate.evaluate: Average Metric: 19.869728051108016 / 35 (56.8%)\n",
      "2025/12/16 17:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 56.77 on minibatch of size 35 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/12/16 17:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77]\n",
      "2025/12/16 17:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23]\n",
      "2025/12/16 17:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 56.23\n",
      "2025/12/16 17:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:51:19 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 23.84 / 35 (68.1%): 100%|██████████| 35/35 [00:37<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:51:57 INFO dspy.evaluate.evaluate: Average Metric: 23.839503800001268 / 35 (68.1%)\n",
      "2025/12/16 17:51:57 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 68.11 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 17:51:57 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11]\n",
      "2025/12/16 17:51:57 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23]\n",
      "2025/12/16 17:51:57 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 56.23\n",
      "2025/12/16 17:51:57 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:51:57 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 22.12 / 35 (63.2%): 100%|██████████| 35/35 [00:43<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:52:40 INFO dspy.evaluate.evaluate: Average Metric: 22.116801438694285 / 35 (63.2%)\n",
      "2025/12/16 17:52:40 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 63.19 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/12/16 17:52:40 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19]\n",
      "2025/12/16 17:52:40 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23]\n",
      "2025/12/16 17:52:40 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 56.23\n",
      "2025/12/16 17:52:40 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:52:40 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.88 / 35 (56.8%): 100%|██████████| 35/35 [00:47<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:53:28 INFO dspy.evaluate.evaluate: Average Metric: 19.877225795185158 / 35 (56.8%)\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 56.79 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79]\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23]\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 56.23\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 23 - Full Evaluation =====\n",
      "2025/12/16 17:53:28 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 68.11) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 95.69 / 160 (59.8%): 100%|██████████| 160/160 [01:40<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:55:08 INFO dspy.evaluate.evaluate: Average Metric: 95.689302150198 / 160 (59.8%)\n",
      "2025/12/16 17:55:08 INFO dspy.teleprompt.mipro_optimizer_v2: \u001B[92mNew best full eval score!\u001B[0m Score: 59.81\n",
      "2025/12/16 17:55:08 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81]\n",
      "2025/12/16 17:55:08 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.81\n",
      "2025/12/16 17:55:08 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/12/16 17:55:08 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/16 17:55:08 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 8 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 20.19 / 35 (57.7%): 100%|██████████| 35/35 [00:42<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:55:51 INFO dspy.evaluate.evaluate: Average Metric: 20.193079991579914 / 35 (57.7%)\n",
      "2025/12/16 17:55:51 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.69 on minibatch of size 35 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 17:55:51 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69]\n",
      "2025/12/16 17:55:51 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81]\n",
      "2025/12/16 17:55:51 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.81\n",
      "2025/12/16 17:55:51 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:55:51 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 9 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 21.10 / 35 (60.3%): 100%|██████████| 35/35 [00:42<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:56:33 INFO dspy.evaluate.evaluate: Average Metric: 21.104435920472095 / 35 (60.3%)\n",
      "2025/12/16 17:56:33 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.3 on minibatch of size 35 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/12/16 17:56:33 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3]\n",
      "2025/12/16 17:56:33 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81]\n",
      "2025/12/16 17:56:33 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.81\n",
      "2025/12/16 17:56:33 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:56:33 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 10 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.75 / 35 (56.4%): 100%|██████████| 35/35 [00:41<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:57:15 INFO dspy.evaluate.evaluate: Average Metric: 19.74756486658851 / 35 (56.4%)\n",
      "2025/12/16 17:57:15 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 56.42 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/12/16 17:57:15 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42]\n",
      "2025/12/16 17:57:15 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81]\n",
      "2025/12/16 17:57:15 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.81\n",
      "2025/12/16 17:57:15 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:57:15 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 11 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.07 / 35 (54.5%): 100%|██████████| 35/35 [00:34<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:57:50 INFO dspy.evaluate.evaluate: Average Metric: 19.06859683486066 / 35 (54.5%)\n",
      "2025/12/16 17:57:50 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 54.48 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 9'].\n",
      "2025/12/16 17:57:50 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48]\n",
      "2025/12/16 17:57:50 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81]\n",
      "2025/12/16 17:57:50 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.81\n",
      "2025/12/16 17:57:50 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:57:50 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 12 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 21.07 / 35 (60.2%): 100%|██████████| 35/35 [00:01<00:00, 21.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:57:51 INFO dspy.evaluate.evaluate: Average Metric: 21.066593729740685 / 35 (60.2%)\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.19 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19]\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81]\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.81\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 23 - Full Evaluation =====\n",
      "2025/12/16 17:57:51 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 64.45) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 97.56 / 160 (61.0%): 100%|██████████| 160/160 [01:47<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:59:39 INFO dspy.evaluate.evaluate: Average Metric: 97.56216202908269 / 160 (61.0%)\n",
      "2025/12/16 17:59:39 INFO dspy.teleprompt.mipro_optimizer_v2: \u001B[92mNew best full eval score!\u001B[0m Score: 60.98\n",
      "2025/12/16 17:59:39 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98]\n",
      "2025/12/16 17:59:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 17:59:39 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/12/16 17:59:39 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/16 17:59:39 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 14 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 20.63 / 35 (58.9%): 100%|██████████| 35/35 [00:02<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 17:59:42 INFO dspy.evaluate.evaluate: Average Metric: 20.62872886582464 / 35 (58.9%)\n",
      "2025/12/16 17:59:42 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 58.94 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 17:59:42 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94]\n",
      "2025/12/16 17:59:42 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98]\n",
      "2025/12/16 17:59:42 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 17:59:42 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 17:59:42 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 15 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 20.18 / 35 (57.7%): 100%|██████████| 35/35 [00:47<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:00:29 INFO dspy.evaluate.evaluate: Average Metric: 20.181681624595367 / 35 (57.7%)\n",
      "2025/12/16 18:00:29 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.66 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 10'].\n",
      "2025/12/16 18:00:29 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66]\n",
      "2025/12/16 18:00:29 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98]\n",
      "2025/12/16 18:00:29 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:00:29 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:00:29 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 16 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 21.81 / 35 (62.3%): 100%|██████████| 35/35 [00:36<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:01:06 INFO dspy.evaluate.evaluate: Average Metric: 21.805854852378438 / 35 (62.3%)\n",
      "2025/12/16 18:01:06 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.3 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 18:01:06 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66, 62.3]\n",
      "2025/12/16 18:01:06 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98]\n",
      "2025/12/16 18:01:06 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:01:06 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:01:06 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 17 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.98 / 35 (57.1%): 100%|██████████| 35/35 [00:40<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:01:47 INFO dspy.evaluate.evaluate: Average Metric: 19.97927249111863 / 35 (57.1%)\n",
      "2025/12/16 18:01:47 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.08 on minibatch of size 35 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 8'].\n",
      "2025/12/16 18:01:47 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66, 62.3, 57.08]\n",
      "2025/12/16 18:01:47 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98]\n",
      "2025/12/16 18:01:47 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:01:47 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:01:47 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 18 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 22.87 / 35 (65.4%): 100%|██████████| 35/35 [00:41<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:02:28 INFO dspy.evaluate.evaluate: Average Metric: 22.874626288047256 / 35 (65.4%)\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 65.36 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66, 62.3, 57.08, 65.36]\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98]\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 19 / 23 - Full Evaluation =====\n",
      "2025/12/16 18:02:28 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 65.36) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 96.83 / 160 (60.5%): 100%|██████████| 160/160 [01:44<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:04:12 INFO dspy.evaluate.evaluate: Average Metric: 96.83368005552488 / 160 (60.5%)\n",
      "2025/12/16 18:04:12 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98, 60.52]\n",
      "2025/12/16 18:04:12 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:04:12 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/12/16 18:04:12 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/16 18:04:12 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 20 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.47 / 35 (55.6%): 100%|██████████| 35/35 [00:38<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:04:51 INFO dspy.evaluate.evaluate: Average Metric: 19.47227627767913 / 35 (55.6%)\n",
      "2025/12/16 18:04:51 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.64 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 7'].\n",
      "2025/12/16 18:04:51 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66, 62.3, 57.08, 65.36, 55.64]\n",
      "2025/12/16 18:04:51 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98, 60.52]\n",
      "2025/12/16 18:04:51 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:04:51 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:04:51 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 21 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 19.99 / 35 (57.1%): 100%|██████████| 35/35 [00:19<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:05:11 INFO dspy.evaluate.evaluate: Average Metric: 19.987004875081936 / 35 (57.1%)\n",
      "2025/12/16 18:05:11 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.11 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/12/16 18:05:11 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66, 62.3, 57.08, 65.36, 55.64, 57.11]\n",
      "2025/12/16 18:05:11 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98, 60.52]\n",
      "2025/12/16 18:05:11 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:05:11 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:05:11 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 22 / 23 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 20.23 / 35 (57.8%): 100%|██████████| 35/35 [00:39<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:05:50 INFO dspy.evaluate.evaluate: Average Metric: 20.229472275943348 / 35 (57.8%)\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.8 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 11'].\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [64.45, 56.77, 68.11, 63.19, 56.79, 57.69, 60.3, 56.42, 54.48, 60.19, 58.94, 57.66, 62.3, 57.08, 65.36, 55.64, 57.11, 57.8]\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98, 60.52]\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 23 / 23 - Full Evaluation =====\n",
      "2025/12/16 18:05:50 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 63.19) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 96.59 / 160 (60.4%): 100%|██████████| 160/160 [01:43<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/16 18:07:34 INFO dspy.evaluate.evaluate: Average Metric: 96.58909676931971 / 160 (60.4%)\n",
      "2025/12/16 18:07:34 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [56.23, 59.81, 60.98, 60.52, 60.37]\n",
      "2025/12/16 18:07:34 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.98\n",
      "2025/12/16 18:07:34 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/12/16 18:07:34 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/16 18:07:34 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 60.98!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T17:25:09.060871Z",
     "start_time": "2025-12-16T17:25:03.533751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline = rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(baseline.response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command + Tab will not switch to minimized windows directly. To bring back a minimized app, first ensure the app is not minimized. If it is minimized, you'll need to select it from the application list without the Command key being released to allow focus transition. Additionally, you could alter your System Preferences in Mission Control to include options for managing minimized applications.\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T17:25:32.144571Z",
     "start_time": "2025-12-16T17:25:23.678041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = optimized_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(pred.response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using Command + Tab on macOS, it's important to note that this shortcut does not reactivate hidden or minimized windows by default. Instead, it allows you to cycle through open applications that are active or visible. If you want to ensure that you can switch to a minimized app, you should first follow these steps: \n",
      "\n",
      "1. While holding the Command key, tap the Tab key until you highlight the app you want to switch to.\n",
      "2. Before releasing the Command key, press and hold the Option key. This allows the app to take focus even if it was minimized before.\n",
      "3. You will need to switch focus to another app entirely before returning to your minimized app by switching back while continuing to hold the keys.\n",
      "\n",
      "If you have the \"When switching to an application, switch to a Space with open windows for the application\" option unchecked in Mission Control under System Preferences, this can help this switching process as well. \n",
      "\n",
      "Also, consider using the shortcut Command + Option + H + M to hide all other apps and minimize your most recently used app, which might be an alternative way to manage your workspace more effectively.\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T17:27:06.271076Z",
     "start_time": "2025-12-16T17:27:06.265Z"
    }
   },
   "cell_type": "code",
   "source": "dspy.inspect_history()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-12-16T18:25:32.142559]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): \n",
      "2. `question` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `response` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Imagine you are a tech support specialist assisting a user with a critical technical issue on a macOS or Unix/Linux system. Your task is to provide a comprehensive explanation to their query based on the provided context. Using the fields `context` and `question`, think critically about the information, and produce a detailed `response` that not only answers the user's question but also includes practical advice or troubleshooting tips they can apply immediately. Ensure your response empowers the user to understand their situation better and take the necessary steps to resolve their issue.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "locate vs find: usage, pros and cons of each other\n",
      "\n",
      "\n",
      "\u001B[31mAssistant message:\u001B[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example. \n",
      "\n",
      "[[ ## response ## ]]\n",
      "Locate uses a prebuilt database, which should be regularly updated, while find iterates over a filesystem to locate files; locate has one big advantage over find: speed, but it can be inaccurate if the database has not been updated. \n",
      "\n",
      "Locate typically lists all files matching your pattern, leaving it to grep filtering and similar to cut this down to size, while find can do things to files it finds, one such operator being -exec.  \n",
      "\n",
      "Find is not intuitive for a novice or occasional user of Unix, and, historically, some versions didn't have the -print option as the default; locate is less flexible, but far more intuitive to use in the common case.\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «The self-documenting code best practice is not a mainstream consensus - while it is widely acceptable that easy-to-understand code is better than cryptic code, not everyone agrees on whether or not complicated code must always be refactored or if its OK to comment it. But this debate is about pieces of code that are difficult to understand - and you are talking about commenting each and every line of code. Lines of code that are difficult to understand should be very rare - if most of your lines are complicated like this than you are overusing smart-ass one-liners and you should stop! Sure, a lines role may be a bit hard to understand sometimes, but thats its role in the context of a bigger piece of code - what the line itself does is almost always straightforward. So, you should not be commenting lines - you should be commenting pieces of code. Specific comments may be placed near the lines they refer to, but these comments are still referring to the bigger code. They do not describe what/why the line does - they describe what it does in that code and/or why is it needed in that code. So, it is not lines that should be commented but bigger pieces of code. Should every piece of code be commented? No. Harold Abelson said that Programs must be written for people to read, and only incidentally for machines to execute. But comments are only for people to read - the machine does not execute them. If your coding standards force writing redundant comments on every line/piece-of-code you are not just burdening the developer who needs to write them - you are also burdening the developers who will have to read them! This is a problem, because when most of the comments you read are redundant, you stop paying attention to them(because you know theyll just be redundant junk), and then youll miss out the important comments. So I say - write only the important comment, so that when someone see a comment in the code theyll know its worth to read it in order to understand the code.»\n",
      "[2] «Comment every line of code? No. The purpose of the others rules you talk about is precisely to avoid that. Comments to a readable code are at best redundant, and at worst will throw a reader off looking for the non-existent purpose of the comment. If you comment whole code which is self-explanatory, you double the amount of reading with out giving any additional information. I am not sure if such redundancy would pay off. One can imagine a reviewer saying that 42 says display_error while the comment says displays warning. But imagine a change in the code. You have two places to correct now. It becomes clear this style has copy-paste negatives. I would say that optimally the code should not need any comments, other than documentation. There are styles which go total opposite, if you have doubts about line its either: The code is too complicated and should be refactored into a function with a name with a semantic meaning for the portion of code. Be it a complex if or a portion of an algorithm. (Or a clever LINQ one-liner) If it cannot be simplified, you dont know enough idioms of language. (I personally am not a fanatic of strict no-comments rule, but I find it as a good initial mindset to go by.) Given all this is it even possible to write good coding standards that capture this idea? As for the process. Our standard gave quite much credit to the reviewer. Assuming they are not malicious this works good. When he asks What is variable o?, you rename it. If he asks what does this block do, either refactor or comment. If there was a debate whether something is clear or not, if the reviewer did not get it, then by definition it is not. On very rare occasions there was something like 2nd opinion from few friends. On average, you should get a code understandable by the average programmer in the team. IMO it keeps off the redundant information, and ensures that the code is understandable by at least one other member of the team, which we found a good optimum. Also, there were no absolutes. Though we were a small group. Its easy to find consensus in group of 5.»\n",
      "[3] «I dont see a compelling argument here for non-line comments, and personally I cannot remember the last time Ive used one. Comments in and of themselves are decreasing in frequency, and should be focused on explaining why code is doing stuff. That is done using line comments. Leaving commented code in your codebase is an anti-best practice with the ubiquity of source control systems (and distributed systems make it even easier). Multi line comments arent going to make or break your programming language. Were not using stone knives and bearskins anymore. When I first started, not a few programmers used vi (not even vim) because things like syntax highlighting were a crutch. Programmers use tools, take advantage of that.»\n",
      "[4] «If the code is trivial, it doesnt need an explanatory comment. If the code is non-trivial, the explanatory comment will most likely also be non-trivial. Now, the trouble with non-trivial natural language is that many of us are not very good at reading it or writing it. Im sure your written communication skills are excellent, but nevertheless someone with a lesser grasp of written language might misunderstand your words. If you try very hard to write natural language that cannot be misinterpreted you end up with something like a legal document (and as we all know those are more verbose and difficult to understand than code). Code should be the most concise description of your logic, and there shouldnt be much debate about the meaning of your code because your compiler and platform have the final say. Personally I wouldnt say that you should never write a comment. Only that you should consider why your code needs a comment, and how you might fix that. This seems to be a common theme in answers here.»\n",
      "[5] «The principal purpose of code is commanding a computer to do something, so a good comment is never a substitute for good code because comments cant be executed. That being said, comments in the source are one form of documentation for other programmers (including yourself). If the comments are about more abstract issues than what the code is doing at every step, youre doing better than average. That level of abstraction varies with the tool youre using. Comments accompanying assembly language routines generally have a lower level of abstraction than, for example, this APL A←0⋄A⊣{2⊤⍵:1+3×⍵⋄⍵÷2}⍣{⍺=A+←1}⎕. I think that would probably merit a comment about the problem its intended to solve, hmmm?»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "coding standard for clarity: comment every line of code?\n",
      "\n",
      "\n",
      "\u001B[31mAssistant message:\u001B[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "The context presents a debate about the necessity and effectiveness of commenting every line of code. It highlights the opinion that commenting every line may not be beneficial, as it can lead to redundancy and potentially confuse the reader. Instead, it advocates for the practice of writing comments that clarify the purpose of larger code blocks rather than each individual line. The arguments suggest a preference for clean, understandable code that does not require constant comments, allowing comments to serve as supplementary explanations for complex logic or abstract concepts. Therefore, a coding standard that mandates commenting every line would likely hinder rather than help code clarity.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "Commenting every line of code is generally not advisable as it can lead to redundancy, confusion, and ultimately detract from the clarity of the code. Instead, the focus should be on writing clear and understandable code, where comments explain larger sections of code or provide context for complex logic. Good coding practice promotes the idea that if the code is self-explanatory, comments might not be necessary. Thus, a strong coding standard would encourage meaningful comments only when necessary, rather than requiring line-by-line commentary.\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «If you hold command and quickly tap tab you will cycle between your two most recently used applications without bringing up the heads up display. If you want to use a third party app Witch has options to ignore minimized windows.»\n",
      "[2] «Try this: On your Mac, Navigate to System Preferences Go to Mission Control Uncheck When switching to an application, switch to a Space with open windows for the application Try using the cmd+tab now.»\n",
      "[3] «Command + Tab until you get the apps icon. Before releasing the Command key, press and hold the Option key. You must switch to another app and let it take focus first. In other words, you cant just Command + Tab to another app and before actually selecting that app (by releasing the Command and Tab keys), switch right back to your minimized app, which you might attempt to do if you minimized it by accident or just simply changed your mind shortly after minimizing. Both the Command and left Option keys must be pressed on the same side (left or right) of the keyboard.»\n",
      "[4] «Command-option-click does not hide all open windows. As stated in the linked Apple article, this command hides all other open items leaving the clicked item visible. It is possible to hide all other items and minimize your most recently item with command+option+h+m. This is a combination of command+option+h - Hide Others and command+option+m - Minimize.»\n",
      "[5] «Cmd+` of course will work but I do it in different way. I do the Cmd+tab when i do that, I dont release the Cmd so the opened apps are still visible. While it is visible, I will press ▼. This way it will show all your opened application with animated displayed so you can navigate using arrow keys.»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "cmd+tab does not work on hidden or minimized windows\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m[[ ## reasoning ## ]]\n",
      "The context provides information regarding application switching on macOS using the Command + Tab shortcut. It explains how Command + Tab allows cycling through recently used applications but does not activate hidden or minimized windows unless specific sequences are followed. The advice includes using the Option key simultaneously to maintain focus on a minimized window or managing window visibility through different shortcuts. Therefore, the primary issue addressed is that merely using Command + Tab will not bring a minimized or hidden window to the forefront unless specific conditions are met.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "When using Command + Tab on macOS, it's important to note that this shortcut does not reactivate hidden or minimized windows by default. Instead, it allows you to cycle through open applications that are active or visible. If you want to ensure that you can switch to a minimized app, you should first follow these steps: \n",
      "\n",
      "1. While holding the Command key, tap the Tab key until you highlight the app you want to switch to.\n",
      "2. Before releasing the Command key, press and hold the Option key. This allows the app to take focus even if it was minimized before.\n",
      "3. You will need to switch focus to another app entirely before returning to your minimized app by switching back while continuing to hold the keys.\n",
      "\n",
      "If you have the \"When switching to an application, switch to a Space with open windows for the application\" option unchecked in Mission Control under System Preferences, this can help this switching process as well. \n",
      "\n",
      "Also, consider using the shortcut Command + Option + H + M to hide all other apps and minimize your most recently used app, which might be an alternative way to manage your workspace more effectively.\n",
      "\n",
      "[[ ## completed ## ]]\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###Improved Instruction\n",
    "\n",
    "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T17:35:08.881368Z",
     "start_time": "2025-12-16T17:35:08.875019Z"
    }
   },
   "cell_type": "code",
   "source": "optimized_rag",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respond.predict = Predict(StringSignature(context, question -> reasoning, response\n",
       "    instructions=\"Imagine you are a tech support specialist assisting a user with a critical technical issue on a macOS or Unix/Linux system. Your task is to provide a comprehensive explanation to their query based on the provided context. Using the fields `context` and `question`, think critically about the information, and produce a detailed `response` that not only answers the user's question but also includes practical advice or troubleshooting tips they can apply immediately. Ensure your response empowers the user to understand their situation better and take the necessary steps to resolve their issue.\"\n",
       "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    response = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Response:', 'desc': '${response}'})\n",
       "))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T17:36:01.146072Z",
     "start_time": "2025-12-16T17:36:01.139506Z"
    }
   },
   "cell_type": "code",
   "source": "rag",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respond.predict = Predict(StringSignature(context, question -> reasoning, response\n",
       "    instructions='Given the fields `context`, `question`, produce the fields `response`.'\n",
       "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    response = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Response:', 'desc': '${response}'})\n",
       "))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Synthetic Dataset Generation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
