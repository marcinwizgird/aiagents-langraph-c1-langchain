{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:05.775002Z",
     "start_time": "2025-11-29T18:06:05.770035Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda, RunnableParallel\n",
    "from langchain_core.tracers.context import collect_runs\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:07.711565Z",
     "start_time": "2025-11-29T18:06:07.708426Z"
    }
   },
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:09.080742Z",
     "start_time": "2025-11-29T18:06:07.754589Z"
    }
   },
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining invocations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:09.687068Z",
     "start_time": "2025-11-29T18:06:09.682401Z"
    }
   },
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:11.141683Z",
     "start_time": "2025-11-29T18:06:11.136288Z"
    }
   },
   "source": [
    "parser = StrOutputParser()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:25.137251Z",
     "start_time": "2025-11-29T18:06:23.400492Z"
    }
   },
   "source": [
    "parser.invoke(\n",
    "    llm.invoke(\n",
    "        prompt.invoke(\n",
    "            {\"topic\": \"Python\"}\n",
    "        )\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnables can be \n",
    "- executed\n",
    "    - invoke(), \n",
    "    - batch() \n",
    "    - and stream()\n",
    "- inspected,\n",
    "- and composed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:06:58.536138Z",
     "start_time": "2025-11-29T18:06:58.533264Z"
    }
   },
   "source": [
    "runnables = [prompt, llm, parser]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute methods**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:07:02.533259Z",
     "start_time": "2025-11-29T18:07:02.528821Z"
    }
   },
   "source": [
    "for runnable in runnables:\n",
    "    print(f\"{repr(runnable).split('(')[0]}\")\n",
    "    print(f\"\\tINVOKE: {repr(runnable.invoke)}\")\n",
    "    print(f\"\\tBATCH: {repr(runnable.batch)}\")\n",
    "    print(f\"\\tSTREAM: {repr(runnable.stream)}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate\n",
      "\tINVOKE: <bound method BasePromptTemplate.invoke of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\tBATCH: <bound method Runnable.batch of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\tSTREAM: <bound method Runnable.stream of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\n",
      "ChatOpenAI\n",
      "\tINVOKE: <bound method BaseChatModel.invoke of ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018695F66270>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018695F66CF0>, root_client=<openai.OpenAI object at 0x0000018695F64050>, root_async_client=<openai.AsyncOpenAI object at 0x0000018695F66A50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)>\n",
      "\tBATCH: <bound method Runnable.batch of ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018695F66270>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018695F66CF0>, root_client=<openai.OpenAI object at 0x0000018695F64050>, root_async_client=<openai.AsyncOpenAI object at 0x0000018695F66A50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)>\n",
      "\tSTREAM: <bound method BaseChatModel.stream of ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018695F66270>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018695F66CF0>, root_client=<openai.OpenAI object at 0x0000018695F64050>, root_async_client=<openai.AsyncOpenAI object at 0x0000018695F66A50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)>\n",
      "\n",
      "StrOutputParser\n",
      "\tINVOKE: <bound method BaseOutputParser.invoke of StrOutputParser()>\n",
      "\tBATCH: <bound method Runnable.batch of StrOutputParser()>\n",
      "\tSTREAM: <bound method Runnable.stream of StrOutputParser()>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:07:05.279289Z",
     "start_time": "2025-11-29T18:07:05.264701Z"
    }
   },
   "source": [
    "for runnable in runnables:\n",
    "    print(f\"{repr(runnable).split('(')[0]}\")\n",
    "    print(f\"\\tINPUT: {repr(runnable.get_input_schema())}\")\n",
    "    print(f\"\\tOUTPUT: {repr(runnable.get_output_schema())}\")\n",
    "    print(f\"\\tCONFIG: {repr(runnable.config_schema())}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate\n",
      "\tINPUT: <class 'langchain_core.utils.pydantic.PromptInput'>\n",
      "\tOUTPUT: <class 'langchain_core.prompts.prompt.PromptTemplateOutput'>\n",
      "\tCONFIG: <class 'langchain_core.utils.pydantic.PromptTemplateConfig'>\n",
      "\n",
      "ChatOpenAI\n",
      "\tINPUT: <class 'langchain_openai.chat_models.base.ChatOpenAIInput'>\n",
      "\tOUTPUT: <class 'langchain_openai.chat_models.base.ChatOpenAIOutput'>\n",
      "\tCONFIG: <class 'langchain_core.utils.pydantic.ChatOpenAIConfig'>\n",
      "\n",
      "StrOutputParser\n",
      "\tINPUT: <class 'langchain_core.output_parsers.string.StrOutputParserInput'>\n",
      "\tOUTPUT: <class 'langchain_core.output_parsers.string.StrOutputParserOutput'>\n",
      "\tCONFIG: <class 'langchain_core.utils.pydantic.StrOutputParserConfig'>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:10:01.129169Z",
     "start_time": "2025-11-29T18:10:01.123930Z"
    }
   },
   "cell_type": "code",
   "source": "runnables[1].config_schema().model_json_schema()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {}, 'title': 'ChatOpenAIConfig', 'type': 'object'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:14:06.737049Z",
     "start_time": "2025-11-29T18:14:04.874744Z"
    }
   },
   "source": [
    "with collect_runs() as run_collection:\n",
    "    result = llm.invoke(\n",
    "        \"Hello\", \n",
    "        config={\n",
    "            'run_name': 'demo_run', \n",
    "            'tags': ['demo', 'lcel'], \n",
    "            'metadata': {'lesson': 2}\n",
    "        }\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:14:06.755285Z",
     "start_time": "2025-11-29T18:14:06.749807Z"
    }
   },
   "source": [
    "run_collection.traced_runs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RunTree(id=8760039a-5207-43e4-901b-eee81a77bd47, name='demo_run', run_type='llm', dotted_order='20251129T181404878182Z8760039a-5207-43e4-901b-eee81a77bd47')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:14:13.168137Z",
     "start_time": "2025-11-29T18:14:13.161674Z"
    }
   },
   "source": [
    "run_collection.traced_runs[0].dict()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': UUID('8760039a-5207-43e4-901b-eee81a77bd47'),\n",
       " 'name': 'demo_run',\n",
       " 'start_time': datetime.datetime(2025, 11, 29, 18, 14, 4, 878182, tzinfo=datetime.timezone.utc),\n",
       " 'run_type': 'llm',\n",
       " 'end_time': datetime.datetime(2025, 11, 29, 18, 14, 6, 734983, tzinfo=datetime.timezone.utc),\n",
       " 'extra': {'invocation_params': {'model': 'gpt-4o-mini',\n",
       "   'model_name': 'gpt-4o-mini',\n",
       "   'stream': False,\n",
       "   'temperature': 0.0,\n",
       "   '_type': 'openai-chat',\n",
       "   'stop': None},\n",
       "  'options': {'stop': None},\n",
       "  'batch_size': 1,\n",
       "  'metadata': {'lesson': 2,\n",
       "   'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.0}},\n",
       " 'error': None,\n",
       " 'serialized': {'lc': 1,\n",
       "  'type': 'constructor',\n",
       "  'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       "  'kwargs': {'model_name': 'gpt-4o-mini',\n",
       "   'temperature': 0.0,\n",
       "   'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']},\n",
       "   'stream_usage': True},\n",
       "  'name': 'ChatOpenAI'},\n",
       " 'events': [{'name': 'start',\n",
       "   'time': datetime.datetime(2025, 11, 29, 18, 14, 4, 878182, tzinfo=datetime.timezone.utc)},\n",
       "  {'name': 'end',\n",
       "   'time': datetime.datetime(2025, 11, 29, 18, 14, 6, 734983, tzinfo=datetime.timezone.utc)}],\n",
       " 'inputs': {'prompts': ['Human: Hello']},\n",
       " 'outputs': {'generations': [[{'text': 'Hello! How can I assist you today?',\n",
       "     'generation_info': {'finish_reason': 'stop', 'logprobs': None},\n",
       "     'type': 'ChatGeneration',\n",
       "     'message': {'lc': 1,\n",
       "      'type': 'constructor',\n",
       "      'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
       "      'kwargs': {'content': 'Hello! How can I assist you today?',\n",
       "       'additional_kwargs': {'refusal': None},\n",
       "       'response_metadata': {'token_usage': {'completion_tokens': 9,\n",
       "         'prompt_tokens': 8,\n",
       "         'total_tokens': 17,\n",
       "         'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "          'audio_tokens': 0,\n",
       "          'reasoning_tokens': 0,\n",
       "          'rejected_prediction_tokens': 0},\n",
       "         'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "        'model_provider': 'openai',\n",
       "        'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "        'system_fingerprint': 'fp_67cf3fed12',\n",
       "        'id': 'chatcmpl-ChJkJp1FFiC0WFDO6uYyZ4QbZWLyX',\n",
       "        'service_tier': 'default',\n",
       "        'finish_reason': 'stop',\n",
       "        'logprobs': None},\n",
       "       'type': 'ai',\n",
       "       'id': 'lc_run--8760039a-5207-43e4-901b-eee81a77bd47-0',\n",
       "       'usage_metadata': {'input_tokens': 8,\n",
       "        'output_tokens': 9,\n",
       "        'total_tokens': 17,\n",
       "        'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "        'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
       "       'tool_calls': [],\n",
       "       'invalid_tool_calls': []}}}]],\n",
       "  'llm_output': {'token_usage': {'completion_tokens': 9,\n",
       "    'prompt_tokens': 8,\n",
       "    'total_tokens': 17,\n",
       "    'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "     'audio_tokens': 0,\n",
       "     'reasoning_tokens': 0,\n",
       "     'rejected_prediction_tokens': 0},\n",
       "    'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "   'model_provider': 'openai',\n",
       "   'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "   'system_fingerprint': 'fp_67cf3fed12',\n",
       "   'id': 'chatcmpl-ChJkJp1FFiC0WFDO6uYyZ4QbZWLyX',\n",
       "   'service_tier': 'default'},\n",
       "  'run': None,\n",
       "  'type': 'LLMResult'},\n",
       " 'reference_example_id': None,\n",
       " 'parent_run_id': None,\n",
       " 'tags': ['demo', 'lcel'],\n",
       " 'attachments': {},\n",
       " 'child_runs': [],\n",
       " 'session_name': 'default',\n",
       " 'session_id': None,\n",
       " 'dotted_order': '20251129T181404878182Z8760039a-5207-43e4-901b-eee81a77bd47',\n",
       " 'trace_id': UUID('8760039a-5207-43e4-901b-eee81a77bd47'),\n",
       " 'dangerously_allow_filesystem': False,\n",
       " 'replicas': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compose Runnables**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:16:20.910686Z",
     "start_time": "2025-11-29T18:16:20.905955Z"
    }
   },
   "source": [
    "chain = RunnableSequence(prompt, llm, parser)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:16:22.799957Z",
     "start_time": "2025-11-29T18:16:22.795735Z"
    }
   },
   "source": [
    "type(chain)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:16:31.857663Z",
     "start_time": "2025-11-29T18:16:30.536234Z"
    }
   },
   "source": [
    "chain.invoke({\"topic\": \"Python\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:16:36.338955Z",
     "start_time": "2025-11-29T18:16:34.507957Z"
    }
   },
   "source": [
    "for chunk in chain.stream({\"topic\": \"Python\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do Python programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:16:54.897578Z",
     "start_time": "2025-11-29T18:16:52.930612Z"
    }
   },
   "source": [
    "chain.batch([\n",
    "    {\"topic\": \"Python\"},\n",
    "    {\"topic\": \"Data\"},\n",
    "    {\"topic\": \"Machine Learning\"},\n",
    "])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!',\n",
       " 'Why did the data break up with the database?\\n\\nBecause it found too many \"issues\" in their relationship!',\n",
       " 'Why did the neural network break up with the decision tree?\\n\\nBecause it found someone with more layers!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:18:33.063366Z",
     "start_time": "2025-11-29T18:18:33.058685Z"
    }
   },
   "cell_type": "code",
   "source": "import grandalf",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:19:51.160261Z",
     "start_time": "2025-11-29T18:19:51.085992Z"
    }
   },
   "source": [
    "chain.get_graph().print_ascii()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install grandalf to draw graphs: `pip install grandalf`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprint_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:523\u001B[39m, in \u001B[36mGraph.print_ascii\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprint_ascii\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    522\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Print the graph as an ASCII art string.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m523\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdraw_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:516\u001B[39m, in \u001B[36mGraph.draw_ascii\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    513\u001B[39m \u001B[38;5;66;03m# Import locally to prevent circular import\u001B[39;00m\n\u001B[32m    514\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgraph_ascii\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m draw_ascii  \u001B[38;5;66;03m# noqa: PLC0415\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m516\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw_ascii\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    517\u001B[39m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43medges\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph_ascii.py:296\u001B[39m, in \u001B[36mdraw_ascii\u001B[39m\u001B[34m(vertices, edges)\u001B[39m\n\u001B[32m    293\u001B[39m xlist: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m] = []\n\u001B[32m    294\u001B[39m ylist: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m] = []\n\u001B[32m--> \u001B[39m\u001B[32m296\u001B[39m sug = \u001B[43m_build_sugiyama_layout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvertices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medges\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m vertex \u001B[38;5;129;01min\u001B[39;00m sug.g.sV:\n\u001B[32m    299\u001B[39m     \u001B[38;5;66;03m# NOTE: moving boxes w/2 to the left\u001B[39;00m\n\u001B[32m    300\u001B[39m     xlist.extend(\n\u001B[32m    301\u001B[39m         (\n\u001B[32m    302\u001B[39m             vertex.view.xy[\u001B[32m0\u001B[39m] - vertex.view.w / \u001B[32m2.0\u001B[39m,\n\u001B[32m    303\u001B[39m             vertex.view.xy[\u001B[32m0\u001B[39m] + vertex.view.w / \u001B[32m2.0\u001B[39m,\n\u001B[32m    304\u001B[39m         )\n\u001B[32m    305\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph_ascii.py:203\u001B[39m, in \u001B[36m_build_sugiyama_layout\u001B[39m\u001B[34m(vertices, edges)\u001B[39m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _HAS_GRANDALF:\n\u001B[32m    202\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInstall grandalf to draw graphs: `pip install grandalf`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[32m    205\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    206\u001B[39m \u001B[38;5;66;03m# Just a reminder about naming conventions:\u001B[39;00m\n\u001B[32m    207\u001B[39m \u001B[38;5;66;03m# +------------X\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    212\u001B[39m \u001B[38;5;66;03m# Y\u001B[39;00m\n\u001B[32m    213\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    215\u001B[39m vertices_ = {id_: Vertex(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m id_, data \u001B[38;5;129;01min\u001B[39;00m vertices.items()}\n",
      "\u001B[31mImportError\u001B[39m: Install grandalf to draw graphs: `pip install grandalf`."
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn any function into a runnable**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:20:15.366666Z",
     "start_time": "2025-11-29T18:20:15.361683Z"
    }
   },
   "source": [
    "def double(x:int)->int:\n",
    "    return 2*x"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:20:16.287358Z",
     "start_time": "2025-11-29T18:20:16.281477Z"
    }
   },
   "source": [
    "runnable = RunnableLambda(double)\n",
    "runnable.invoke(2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Runnables**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:01.309358Z",
     "start_time": "2025-11-29T18:21:01.305797Z"
    }
   },
   "source": [
    "parallel_chain = RunnableParallel(\n",
    "    double=RunnableLambda(lambda x: x * 2),\n",
    "    triple=RunnableLambda(lambda x: x * 3),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:03.232105Z",
     "start_time": "2025-11-29T18:21:03.222643Z"
    }
   },
   "source": [
    "parallel_chain.invoke(3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'double': 6, 'triple': 9}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:04.659205Z",
     "start_time": "2025-11-29T18:21:04.562619Z"
    }
   },
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install grandalf to draw graphs: `pip install grandalf`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mparallel_chain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mprint_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:523\u001B[39m, in \u001B[36mGraph.print_ascii\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprint_ascii\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    522\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Print the graph as an ASCII art string.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m523\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdraw_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:516\u001B[39m, in \u001B[36mGraph.draw_ascii\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    513\u001B[39m \u001B[38;5;66;03m# Import locally to prevent circular import\u001B[39;00m\n\u001B[32m    514\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgraph_ascii\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m draw_ascii  \u001B[38;5;66;03m# noqa: PLC0415\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m516\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw_ascii\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    517\u001B[39m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43medges\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph_ascii.py:296\u001B[39m, in \u001B[36mdraw_ascii\u001B[39m\u001B[34m(vertices, edges)\u001B[39m\n\u001B[32m    293\u001B[39m xlist: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m] = []\n\u001B[32m    294\u001B[39m ylist: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m] = []\n\u001B[32m--> \u001B[39m\u001B[32m296\u001B[39m sug = \u001B[43m_build_sugiyama_layout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvertices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medges\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m vertex \u001B[38;5;129;01min\u001B[39;00m sug.g.sV:\n\u001B[32m    299\u001B[39m     \u001B[38;5;66;03m# NOTE: moving boxes w/2 to the left\u001B[39;00m\n\u001B[32m    300\u001B[39m     xlist.extend(\n\u001B[32m    301\u001B[39m         (\n\u001B[32m    302\u001B[39m             vertex.view.xy[\u001B[32m0\u001B[39m] - vertex.view.w / \u001B[32m2.0\u001B[39m,\n\u001B[32m    303\u001B[39m             vertex.view.xy[\u001B[32m0\u001B[39m] + vertex.view.w / \u001B[32m2.0\u001B[39m,\n\u001B[32m    304\u001B[39m         )\n\u001B[32m    305\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\DATAENLIGHT_AI_LAB\\Lib\\site-packages\\langchain_core\\runnables\\graph_ascii.py:203\u001B[39m, in \u001B[36m_build_sugiyama_layout\u001B[39m\u001B[34m(vertices, edges)\u001B[39m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _HAS_GRANDALF:\n\u001B[32m    202\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInstall grandalf to draw graphs: `pip install grandalf`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[32m    205\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    206\u001B[39m \u001B[38;5;66;03m# Just a reminder about naming conventions:\u001B[39;00m\n\u001B[32m    207\u001B[39m \u001B[38;5;66;03m# +------------X\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    212\u001B[39m \u001B[38;5;66;03m# Y\u001B[39;00m\n\u001B[32m    213\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    215\u001B[39m vertices_ = {id_: Vertex(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m id_, data \u001B[38;5;129;01min\u001B[39;00m vertices.items()}\n",
      "\u001B[31mImportError\u001B[39m: Install grandalf to draw graphs: `pip install grandalf`."
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:08.631084Z",
     "start_time": "2025-11-29T18:21:08.624611Z"
    }
   },
   "source": [
    "prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:09.225860Z",
     "start_time": "2025-11-29T18:21:09.219950Z"
    }
   },
   "source": [
    "llm"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018695F66270>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018695F66CF0>, root_client=<openai.OpenAI object at 0x0000018695F64050>, root_async_client=<openai.AsyncOpenAI object at 0x0000018695F66A50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:10.204900Z",
     "start_time": "2025-11-29T18:21:10.198419Z"
    }
   },
   "source": [
    "parser"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:15.305702Z",
     "start_time": "2025-11-29T18:21:15.298910Z"
    }
   },
   "source": [
    "chain = RunnableSequence(prompt, llm, parser)\n",
    "chain"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018695F66270>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018695F66CF0>, root_client=<openai.OpenAI object at 0x0000018695F64050>, root_async_client=<openai.AsyncOpenAI object at 0x0000018695F66A50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:22.958329Z",
     "start_time": "2025-11-29T18:21:22.953601Z"
    }
   },
   "source": [
    "prompt | llm | parser"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x0000018695F66270>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018695F66CF0>, root_client=<openai.OpenAI object at 0x0000018695F64050>, root_async_client=<openai.AsyncOpenAI object at 0x0000018695F66A50>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:24.291073Z",
     "start_time": "2025-11-29T18:21:24.286471Z"
    }
   },
   "source": [
    "chain = prompt | llm | parser"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:26.599230Z",
     "start_time": "2025-11-29T18:21:25.638921Z"
    }
   },
   "source": [
    "chain.invoke(\n",
    "    {\"topic\": \"computer\"}\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the computer go to therapy?\\n\\nBecause it had too many bytes from its past!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:21:28.000303Z",
     "start_time": "2025-11-29T18:21:27.997592Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
